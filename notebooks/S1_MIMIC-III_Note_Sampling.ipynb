{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Notes from MIMIC-III v1.4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'../../ehr-rwe/')\n",
    "\n",
    "import glob\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "MIMIC_DATASET_ROOT = '/Users/fries/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Annotated Notes\n",
    "\n",
    "We load out set of annotated note IDs to guarantee our unlabeled set is disjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298\n",
      "298\n",
      "CPU times: user 1min, sys: 3.59 s, total: 1min 4s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import collections\n",
    "\n",
    "# load annotated files\n",
    "filelist = glob.glob('/Users/fries/Desktop/npj-dev-surveillance-MIMIC-III/v2/jason/*')\n",
    "filelist = [fp.split(\"/\")[-1].split('.')[0].split('_')\n",
    "            for fp in filelist if fp.split('.')[-1] not in ('ann','conf')]\n",
    "\n",
    "annotations = dict.fromkeys([tuple([int(v) if v != 'nan' else np.nan for v in fp]) for fp in filelist])\n",
    "annotations = {key[0] for key in annotations}\n",
    "print(len(annotations))\n",
    "\n",
    "annotation_subset = {}\n",
    "\n",
    "# dump TSV\n",
    "fname = f\"{MIMIC_DATASET_ROOT}/NOTEEVENTS.csv.gz\"\n",
    "for chunk in pd.read_csv(fname, sep=',', compression='infer', chunksize=10000):\n",
    "    for row in chunk.itertuples():\n",
    "        digest = hashlib.md5(row.TEXT.encode(\"utf-8\")).digest()\n",
    "        #key = (row.ROW_ID, row.SUBJECT_ID, row.HADM_ID, digest)\n",
    "           \n",
    "        if row.ROW_ID in annotations:\n",
    "            annotation_subset[row.ROW_ID] = row\n",
    "        \n",
    "\n",
    "print(len(annotation_subset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_final = glob.glob('/Users/fries/Desktop/npj-subset/v2/jason/*.ann')\n",
    "subset_final = [int(fp.split('/')[-1].split('.')[0].split('_')[0]) for fp in subset_final]\n",
    "\n",
    "with open('/Users/fries/Desktop/RELEASE-NPJ/gold.mimic.row_ids.tsv', 'w') as fp:\n",
    "    for x in sorted(subset_final):\n",
    "        fp.write(f'{x}\\n')\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "with open('/Users/fries/Desktop/RELEASE-NPJ/holdout.mimic.row_ids.tsv', 'w') as fp:\n",
    "    for x in sorted(annotation_subset):\n",
    "        fp.write(f'{x}\\n')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n",
      "[2149, 2149, 1668, 2149, 2097, 2149, 2149, 2149, 2149, 2149, 2149, 2149, 2149, 2149, 2149, 2149, 2149, 2149, 2149, 2149]\n",
      "ERROR - normed date 1532 below 1900\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from prep_mimic import preprocess\n",
    "\n",
    "tsv = []\n",
    "for row_id in annotation_subset:\n",
    "    row = annotation_subset[row_id]\n",
    "    key = (row.ROW_ID, row.SUBJECT_ID, row.HADM_ID)\n",
    "    digest = hashlib.md5(row.TEXT.encode(\"utf-8\")).digest()\n",
    "    chart_date = row.CHARTDATE\n",
    "    chart_time = row.CHARTTIME\n",
    "    category = row.CATEGORY\n",
    "    desc = row.DESCRIPTION\n",
    "    text = row.TEXT\n",
    "    \n",
    "    text = preprocess(text, preserve_offsets=True)\n",
    "    text = text.replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n",
    "    tsv.append((row.ROW_ID, row.SUBJECT_ID, row.HADM_ID, \n",
    "                row.CHARTDATE, row.CHARTTIME, row.CATEGORY, \n",
    "               row.DESCRIPTION, text))\n",
    "    \n",
    "# dump to TSV   \n",
    "outfpath = '/Users/fries/Desktop/RELEASE-NPJ/gold/annotated_corpus.tsv'\n",
    "with open(outfpath, 'w') as fp:\n",
    "    fp.write('\\t'.join(['ROW_ID', 'SUBJECT_ID', 'HADM_ID', \"CHARTDATE\", \n",
    "                        \"CHARTTIME\", \"CATEGORY\", \"DESCRIPTION\", 'TEXT']) + '\\n')\n",
    "    for row in tsv:\n",
    "        row = list(map(str, row))\n",
    "        fp.write('\\t'.join(row) + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Notes\n",
    "\n",
    "We generate a target annotation set by looking for documents where:\n",
    "- Arguments occur within `max_dist` tokens of each other for both `PAIN-AT` and `IMPLANT-COMPLICATION` relations.\n",
    "- Both relation types occur in the same document.\n",
    "\n",
    "Individually, each relational candidate has approx this prevelance rate per 10,000 documents\n",
    "\n",
    "- 2.41% `IMPLANT-COMPLICATION` \n",
    "- 31.1% `PAIN-AT` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from rwe.utils import load_dict\n",
    "\n",
    "def tokenize(s):\n",
    "    toks = re.split(r'''([''' + string.punctuation + ''']|(\\s|\\n)+)''', s)\n",
    "    return [t.strip().lower() for t in toks if t and t.strip()]\n",
    "\n",
    "def match_query(queries, tokens, max_ngrams=4):\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(i+1, min(len(tokens) + 1, i + 1 + max_ngrams)):\n",
    "            t = ' '.join(tokens[i:j])\n",
    "            if t in queries:\n",
    "                return (True, t, i)\n",
    "    return (False, None, -1)\n",
    "    \n",
    "def match_relation(tokens, concept1, concept2, max_dist=25, verbose=False):\n",
    "    h1,t1,i = match_query(concept1, tokens)\n",
    "    if not h1:\n",
    "        return False\n",
    "    h2,t2,j = match_query(concept2, tokens)\n",
    "    if not h2:\n",
    "        return False\n",
    "    if abs(i-j) < max_dist:\n",
    "        if verbose:\n",
    "            print(t1, t2, abs(i-j))\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "root_dir = \"../data/dicts/\"\n",
    "stopwords = {'head'}\n",
    "\n",
    "concepts = {\n",
    "    'implants': load_dict(f'{root_dir}/implants/implants.txt', stopwords=stopwords),\n",
    "    'complications':load_dict(f'{root_dir}/implants/implant_complications.txt'),\n",
    "    'anatomy':load_dict(f'{root_dir}/anatomy/anat.bz2'),\n",
    "    'pain':load_dict(f'{root_dir}/pain/pain.txt')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping 718\n",
      "skipping 765\n",
      "skipping 463\n",
      "skipping 501\n",
      "skipping 510\n",
      "skipping 519\n",
      "skipping 271\n",
      "skipping 361\n",
      "skipping 838\n",
      "skipping 600\n",
      "skipping 1438\n",
      "skipping 990\n",
      "skipping 1108\n",
      "skipping 1254\n",
      "skipping 1711\n",
      "skipping 1519\n",
      "skipping 1574\n",
      "skipping 1584\n",
      "skipping 1679\n",
      "skipping 2322\n",
      "skipping 1946\n",
      "skipping 1960\n",
      "skipping 1725\n",
      "skipping 1757\n",
      "skipping 1843\n",
      "skipping 1880\n",
      "skipping 1885\n",
      "skipping 2495\n",
      "skipping 2364\n",
      "skipping 2604\n",
      "skipping 2899\n",
      "skipping 2902\n",
      "skipping 3641\n",
      "skipping 3643\n",
      "skipping 3674\n",
      "skipping 4199\n",
      "skipping 4074\n",
      "skipping 4082\n",
      "skipping 3533\n",
      "skipping 4262\n",
      "skipping 4410\n",
      "skipping 4656\n",
      "skipping 4898\n",
      "skipping 4935\n",
      "skipping 5094\n",
      "skipping 5167\n",
      "skipping 5040\n",
      "skipping 5370\n",
      "skipping 5425\n",
      "skipping 5432\n",
      "skipping 5433\n",
      "skipping 5557\n",
      "skipping 8689\n",
      "skipping 7123\n",
      "skipping 12268\n",
      "skipping 6599\n",
      "skipping 6645\n",
      "skipping 7317\n",
      "skipping 13167\n",
      "skipping 10433\n",
      "skipping 5928\n",
      "skipping 5766\n",
      "skipping 5782\n",
      "skipping 5789\n",
      "skipping 5811\n",
      "skipping 6693\n",
      "skipping 6699\n",
      "skipping 10338\n",
      "skipping 8038\n",
      "skipping 9362\n",
      "skipping 8079\n",
      "skipping 6945\n",
      "skipping 7793\n",
      "skipping 7804\n",
      "skipping 7811\n",
      "skipping 9887\n",
      "skipping 8477\n",
      "skipping 7929\n",
      "skipping 9270\n",
      "skipping 6801\n",
      "skipping 10057\n",
      "skipping 10734\n",
      "skipping 9167\n",
      "skipping 9191\n",
      "skipping 6830\n",
      "skipping 8929\n",
      "skipping 7666\n",
      "skipping 8305\n",
      "skipping 16295\n",
      "skipping 13561\n",
      "skipping 17310\n",
      "skipping 11233\n",
      "skipping 15625\n",
      "skipping 14212\n",
      "skipping 11118\n",
      "skipping 13884\n",
      "skipping 16481\n",
      "skipping 10982\n",
      "skipping 16269\n",
      "skipping 17242\n",
      "skipping 13406\n",
      "skipping 12147\n",
      "skipping 15032\n",
      "skipping 11709\n",
      "skipping 14036\n",
      "skipping 12801\n",
      "skipping 11827\n",
      "skipping 12691\n",
      "skipping 12027\n",
      "skipping 14114\n",
      "skipping 13038\n",
      "skipping 24274\n",
      "skipping 22934\n",
      "skipping 24347\n",
      "skipping 18971\n",
      "skipping 21609\n",
      "skipping 23025\n",
      "skipping 23500\n",
      "skipping 24237\n",
      "skipping 21786\n",
      "skipping 21790\n",
      "skipping 15909\n",
      "skipping 16834\n",
      "skipping 16877\n",
      "skipping 18669\n",
      "skipping 15790\n",
      "skipping 15923\n",
      "skipping 16923\n",
      "skipping 16932\n",
      "skipping 18751\n",
      "skipping 16640\n",
      "skipping 15970\n",
      "skipping 18759\n",
      "skipping 31369\n",
      "skipping 32438\n",
      "skipping 29588\n",
      "skipping 30946\n",
      "skipping 31435\n",
      "skipping 31217\n",
      "skipping 29758\n",
      "skipping 19532\n",
      "skipping 20086\n",
      "skipping 32540\n",
      "skipping 31299\n",
      "skipping 31850\n",
      "skipping 32723\n",
      "skipping 21850\n",
      "skipping 25018\n",
      "skipping 24498\n",
      "skipping 23714\n",
      "skipping 19420\n",
      "skipping 19430\n",
      "skipping 21984\n",
      "skipping 25035\n",
      "skipping 20705\n",
      "skipping 24867\n",
      "skipping 28492\n",
      "skipping 37693\n",
      "skipping 37074\n",
      "skipping 39760\n",
      "skipping 38007\n",
      "skipping 37760\n",
      "skipping 25496\n",
      "skipping 38975\n",
      "skipping 39327\n",
      "skipping 37545\n",
      "skipping 39011\n",
      "skipping 38574\n",
      "skipping 28381\n",
      "skipping 40555\n",
      "skipping 39592\n",
      "skipping 26559\n",
      "skipping 37617\n",
      "skipping 39961\n",
      "skipping 29066\n",
      "skipping 29032\n",
      "skipping 25243\n",
      "skipping 25263\n",
      "skipping 30763\n",
      "skipping 26190\n",
      "skipping 28875\n",
      "skipping 27162\n",
      "skipping 26845\n",
      "skipping 28895\n",
      "skipping 28905\n",
      "skipping 27673\n",
      "skipping 25944\n",
      "skipping 30189\n",
      "skipping 30655\n",
      "skipping 27695\n",
      "skipping 27718\n",
      "skipping 46672\n",
      "skipping 44076\n",
      "skipping 41522\n",
      "skipping 46254\n",
      "skipping 46898\n",
      "skipping 46902\n",
      "skipping 45183\n",
      "skipping 43340\n",
      "skipping 44176\n",
      "skipping 44189\n",
      "skipping 45811\n",
      "skipping 41603\n",
      "skipping 44603\n",
      "skipping 47050\n",
      "skipping 47067\n",
      "skipping 44896\n",
      "skipping 46557\n",
      "skipping 46752\n",
      "skipping 46758\n",
      "skipping 46349\n",
      "skipping 44709\n",
      "skipping 44714\n",
      "skipping 45780\n",
      "skipping 45623\n",
      "skipping 45625\n",
      "skipping 46820\n",
      "skipping 46851\n",
      "skipping 44977\n",
      "skipping 35812\n",
      "skipping 34051\n",
      "skipping 36177\n",
      "skipping 35579\n",
      "skipping 36673\n",
      "skipping 36728\n",
      "skipping 36492\n",
      "skipping 36514\n",
      "skipping 38320\n",
      "skipping 38845\n",
      "skipping 33604\n",
      "skipping 35878\n",
      "skipping 34231\n",
      "skipping 40377\n",
      "skipping 36873\n",
      "skipping 36284\n",
      "skipping 35733\n",
      "skipping 36912\n",
      "skipping 34962\n",
      "skipping 35947\n",
      "skipping 35996\n",
      "skipping 32913\n",
      "skipping 35497\n",
      "skipping 34379\n",
      "skipping 52413\n",
      "skipping 50963\n",
      "skipping 52060\n",
      "skipping 52462\n",
      "skipping 52481\n",
      "skipping 47643\n",
      "skipping 50796\n",
      "skipping 51717\n",
      "skipping 52828\n",
      "skipping 51903\n",
      "skipping 51074\n",
      "skipping 52347\n",
      "skipping 53130\n",
      "skipping 47523\n",
      "skipping 47546\n",
      "skipping 50819\n",
      "skipping 48164\n",
      "skipping 53100\n",
      "skipping 52754\n",
      "skipping 49832\n",
      "skipping 41256\n",
      "skipping 52962\n",
      "skipping 43923\n",
      "skipping 43938\n",
      "skipping 43943\n",
      "skipping 43966\n",
      "skipping 43177\n",
      "skipping 43890\n",
      "skipping 41411\n",
      "skipping 51855\n",
      "skipping 41305\n",
      "skipping 52976\n",
      "skipping 41337\n",
      "skipping 42343\n",
      "skipping 43246\n",
      "skipping 42126\n",
      "skipping 44242\n",
      "skipping 42931\n",
      "skipping 43641\n",
      "skipping 41145\n",
      "skipping 41173\n",
      "skipping 43440\n",
      "skipping 51759\n",
      "skipping 41986\n",
      "skipping 43805\n",
      "skipping 42987\n",
      "skipping 41192\n",
      "skipping 61483\n",
      "skipping 62053\n",
      "skipping 66182\n",
      "skipping 68629\n",
      "skipping 87119\n",
      "skipping 97468\n",
      "skipping 103537\n",
      "skipping 103053\n",
      "hits 133384\n",
      "CPU times: user 35min 59s, sys: 5.4 s, total: 36min 4s\n",
      "Wall time: 36min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "hits = {}\n",
    "\n",
    "n_samples = None #10000\n",
    "max_dist = 25\n",
    "target = ('implants', 'complications')\n",
    "target = ('anatomy', 'pain')\n",
    "\n",
    "fname = \"/Users/fries/NOTEEVENTS.csv.gz\"\n",
    "for chunk in pd.read_csv(fname, sep=',', compression='infer', chunksize=10000):\n",
    "    for row in chunk.itertuples():\n",
    "        digest = hashlib.md5(row.TEXT.encode(\"utf-8\")).digest()\n",
    "        key = (row.ROW_ID, row.SUBJECT_ID, row.HADM_ID, digest)\n",
    "        \n",
    "        if key[0] in annotation_subset:\n",
    "            print(f'skipping {key[0]}')\n",
    "            continue\n",
    "        \n",
    "        if key in hits:\n",
    "            continue\n",
    "            \n",
    "        toks = tokenize(row.TEXT)\n",
    "        #v = match_relation(toks, concepts['implants'], concepts['complications'], max_dist)\n",
    "        #if not v:\n",
    "        #    continue\n",
    "        v = match_relation(toks, concepts['anatomy'], concepts['pain'], max_dist)\n",
    "        if v:\n",
    "            hits[key] = row.TEXT\n",
    "\n",
    "    if n_samples != None and len(hits) > n_samples:\n",
    "        break\n",
    "\n",
    "print('hits', len(hits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133384"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/fries/Desktop/RELEASE-NPJ/unlabeled.pain.all.mimic.row_ids.tsv','w') as fp:\n",
    "    for key in hits:\n",
    "        fp.write(f\"{key[0]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = '/Users/fries/Desktop/mimic-sample/'\n",
    "\n",
    "subjects = {}\n",
    "for key in hits:\n",
    "    row_id, subject_id, hadm_id, _ = key\n",
    "    if subject_id in subjects:\n",
    "        continue\n",
    "    subjects[subject_id] = 1\n",
    "    fname = f'{outdir}/{row_id}_{subject_id}_{hadm_id}.txt'\n",
    "    with open(fname,'w') as fp:\n",
    "        fp.write(hits[key])\n",
    "print(len(subjects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export DocTimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "s = '2116-02-07 14:08:00'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections \n",
    "\n",
    "doc_times = {}\n",
    "\n",
    "fname = \"/Users/fries/NOTEEVENTS.csv.gz\"\n",
    "for chunk in pd.read_csv(fname, sep=',', compression='infer', chunksize=10000):\n",
    "    for i,row in enumerate(chunk.itertuples()):\n",
    "        chart_ts = str(row.CHARTTIME)\n",
    "        if chart_ts != 'nan':\n",
    "            ts = datetime.strptime(chart_ts, '%Y-%m-%d %H:%M:%S')\n",
    "        else:\n",
    "            chart_ts = str(row.CHARTDATE)\n",
    "            ts = datetime.strptime(chart_ts, '%Y-%m-%d')\n",
    "        key = (row.ROW_ID, row.SUBJECT_ID, row.HADM_ID)\n",
    "        doc_times['_'.join(map(str,key))] = ts\n",
    "\n",
    "print(len(doc_times))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(doc_times, open('/users/fries/desktop/mimic-doctimes.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwe import dataloader\n",
    "\n",
    "inputdir = '/Users/fries/Desktop/sample/'\n",
    "corpus = dataloader(glob.glob(f'{inputdir}/*.json'))\n",
    "print(f'Loaded {len(corpus)} documents')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwe.utils import load_dict\n",
    "from rwe.labelers.taggers import (\n",
    "    ResetTags, RelationTagger, \n",
    "    DictionaryTagger, NegExTagger, HypotheticalTagger, \n",
    "    SectionHeaderTagger, ParentSectionTagger\n",
    ")\n",
    "\n",
    "dict_pain = load_dict('../data/dicts/pain/pain.txt')\n",
    "dict_anat = load_dict('../data/dicts/anatomy/anat.bz2')\n",
    "dict_impl = load_dict('../data/dicts/implants/implants.txt')\n",
    "dict_comp = load_dict('../data/dicts/implants/implant_complications.txt')\n",
    "\n",
    "pipeline = {\n",
    "    # clear any previous runs\n",
    "    \"reset\"        : ResetTags(),\n",
    "    \n",
    "    # concepts\n",
    "    \"pain\"         : DictionaryTagger({'pain': dict_pain}),\n",
    "    \"anatomy\"      : DictionaryTagger({'anatomy': dict_anat}),\n",
    "    \"implant\"      : DictionaryTagger({'implant': dict_impl}),\n",
    "    \"complication\" : DictionaryTagger({'complication': dict_comp}),\n",
    "    \"sections\"     : SectionHeaderTagger(),\n",
    "    \n",
    "    # attributes / primitives\n",
    "    \"negation\"     : NegExTagger(targets=['pain'], data_root=\"../data/dicts/negex/\"),\n",
    "    \"hypothetical\" : HypotheticalTagger(targets=['pain']),\n",
    "    \"headers\"      : ParentSectionTagger(targets=['pain']),\n",
    "    \n",
    "    # relations\n",
    "    \"pain-at\"         : RelationTagger('pain-at', ('pain', 'anatomy')),\n",
    "    \"complication-at\" : RelationTagger('comp-at', ('complication', 'implant')),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from rwe.labelers import TaggerPipelineServer\n",
    "\n",
    "tagger = TaggerPipelineServer(num_workers=4)\n",
    "documents = tagger.apply(pipeline, [corpus])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwe.utils import build_candidate_set\n",
    "\n",
    "Xs_pain_at = build_candidate_set(documents[0], \"pain-at\")\n",
    "Xs_comp_at = build_candidate_set(documents[0], \"comp-at\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_relation_args(relations):\n",
    "    return set([s for x in relations for s in x])\n",
    "    \n",
    "pain_at_spans = collapse_relation_args(Xs_pain_at)\n",
    "comp_at_spans = collapse_relation_args(Xs_comp_at)\n",
    "\n",
    "doc_span_index = collections.defaultdict(set)\n",
    "for s in comp_at_spans:\n",
    "    doc_span_index[s.sentence.document.name].add(s)\n",
    "for s in pain_at_spans:\n",
    "    doc_span_index[s.sentence.document.name].add(s)\n",
    "    \n",
    "n = 0\n",
    "for doc_name in doc_span_index:\n",
    "    print(doc_name, len(doc_span_index[doc_name]))\n",
    "    n += len(doc_span_index[doc_name])\n",
    "print(n)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = '/Users/fries/Desktop/mimic-sample-brat/'\n",
    "etype = 'Concept'\n",
    "\n",
    "for doc_name in doc_span_index:\n",
    "    outfname = f'{outdir}/{doc_name}.ann'\n",
    "    with open(outfname, 'w') as fp:\n",
    "        items = set()\n",
    "        for i,s in enumerate(doc_span_index[doc_name]):\n",
    "            # T8\tConcept 468 479;480 489\tright lower extremity\n",
    "            multi_spans = []\n",
    "            start = s.abs_char_start\n",
    "            if '\\n' in s.text:\n",
    "                toks = s.text.split(\"\\n\")\n",
    "                for t in toks:\n",
    "                    multi_spans.append((start, start + len(t)))\n",
    "                    start += len(t) + 1\n",
    "                \n",
    "                span_str = [f'{span[0]} {span[1]}' for span in multi_spans]\n",
    "                anno = (etype, \" \", \";\".join(span_str), \"\\t\", s.text.replace(\"\\n\", \" \"))\n",
    "            else:\n",
    "                anno = (etype, \" \", f\"{s.abs_char_start} {s.abs_char_end+1}\", \"\\t\", s.text.replace(\"\\n\", \" \"))\n",
    "                \n",
    "            items.add(anno)\n",
    "            \n",
    "        for i,s in enumerate(sorted(items,key=lambda x:x[1], reverse=0)):\n",
    "            anno = f'T{i+1}\\t{\"\".join(s)}'\n",
    "            print(anno)\n",
    "            fp.write(anno + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.7 (bert)",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
