{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Training Sets with Weak Supervision\n",
    "In this tutorial, we'll build a `Pain-At` relation training set using weakly superivsed methods. This notebook covers: \n",
    "- Loading pre-processed documents\n",
    "- Generating relational candidates \n",
    "- Applying labeling functions\n",
    "- Training a Snorkel Label Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'../../ehr-rwe/')\n",
    "\n",
    "import glob\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load MIMIC-III Documents\n",
    "\n",
    "This notebook assumes documents have already been preprocessed and dumped into JSON format. We created a small annotated dataset using MIMIC-III patient notes. See the `XXXX.ipynb` notebook for instructions on creating the required JSON files \n",
    "\n",
    "You will need access to MIMIC-III data to run this notebook using our tutorial annotations.  See https://mimic.physionet.org/gettingstarted/access/\n",
    "\n",
    "See `preprocessing/README.md` for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 55 documents\n",
      "Loaded 1322 documents\n"
     ]
    }
   ],
   "source": [
    "from rwe import dataloader\n",
    "\n",
    "inputdir = '/Users/fries/Desktop/foobar/output/'\n",
    " \n",
    "corpus = [\n",
    "    dataloader([f'{inputdir}/mimic_gold.json']), \n",
    "    dataloader([f'{inputdir}/mimic_unlabeled.json'])\n",
    "]\n",
    "\n",
    "for split in corpus:\n",
    "    print(f'Loaded {len(split)} documents')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Candidates\n",
    "\n",
    "This is an example pipeline for generating `Pain-At` relation candidates. Relations are defined as a tuple $k$ entity spans. For simplicity's sake, we consider binary relations between all `Anatomy` and `Pain` entity pairs found within the same sentence. Entities can be tagged using a clinical named entity recognition (NER) model if available. Here we use a dictionary-based method to tag our initial `Anatomy` and `Pain` entities. \n",
    "\n",
    "### Clinical Text Markup\n",
    "When writing labeling functions, it's conveinant to have access to document markup and other metadata. For example, we might want to know what document section we are currently in (e.g., Past Medical History) or if we have temporal information above an event, such as a data of occurence, we might want to incorproate that information into our labeling heuristics. \n",
    "\n",
    "### Timing Benchmarks \n",
    "\n",
    "- 50,000 MIMIC-III documents\n",
    "- 4 core MacBook Pro 2.5Ghz mid-2015\n",
    "\n",
    "| N Documents   | N Cores | Time |\n",
    "|---------------|---------|----------------|\n",
    "| 299           | 4       | 17 seconds |\n",
    "| 10,000        | 4       | 4 minutes  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwe.utils import load_dict\n",
    "from rwe.labelers.taggers import (\n",
    "    ResetTags, RelationTagger, \n",
    "    DictionaryTagger, NegExTagger, HypotheticalTagger, HistoricalTagger,\n",
    "    SectionHeaderTagger, ParentSectionTagger,\n",
    "    DocTimeTagger, MappedDocTimeTagger, \n",
    "    Timex3Tagger, Timex3NormalizerTagger, TimeDeltaTagger,\n",
    ")\n",
    "\n",
    "dict_pain = load_dict('../data/dicts/pain/pain.txt')\n",
    "dict_anat = load_dict('../data/dicts/anatomy/anat.bz2')\n",
    "\n",
    "target_entities = ['pain']\n",
    "\n",
    "# NOTE: Pipelines are *order dependant* as normalizers and attribute taggers assume\n",
    "# the existence of certain concept targets (e.g., Timex3Normalizer requires timex3 entities)\n",
    "pipeline = {\n",
    "    # 1. Clear any previous runs\n",
    "    \"reset\"        : ResetTags(),\n",
    "    \n",
    "    # 2. Clinical concepts\n",
    "    \"concepts\"  : DictionaryTagger({'pain': dict_pain, 'anatomy': dict_anat}),\n",
    "    \"headers\"   : SectionHeaderTagger(),\n",
    "    \"timex3\"    : Timex3Tagger(),\n",
    "    \n",
    "    # 3. Normalize datetimes\n",
    "    \"doctimes\"  : DocTimeTagger(prop='CHARTDATE'),\n",
    "    \"normalize\" : Timex3NormalizerTagger(),\n",
    "    \n",
    "    # 4. Concept attributes\n",
    "    \"section\"      : ParentSectionTagger(targets=target_entities),\n",
    "    \"tdelta\"       : TimeDeltaTagger(targets=target_entities),\n",
    "    \"negation\"     : NegExTagger(targets=target_entities, data_root=\"../data/dicts/negex/\"),\n",
    "    \"hypothetical\" : HypotheticalTagger(targets=target_entities),\n",
    "    'historical'   : HistoricalTagger(targets=target_entities),\n",
    "    \n",
    "    # 5. Extract relation candidates\n",
    "    \"pain-at\"      : RelationTagger('pain-at', ('pain', 'anatomy'))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto block size=345\n",
      "Partitioned into 4 blocks, [342 345] sizes\n",
      "CPU times: user 2.38 s, sys: 394 ms, total: 2.77 s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from rwe.labelers import TaggerPipelineServer\n",
    "\n",
    "tagger = TaggerPipelineServer(num_workers=4)\n",
    "documents = tagger.apply(pipeline, corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for split in documents:\n",
    "#     for doc in split:\n",
    "#         #print(doc.name)\n",
    "   \n",
    "#         for sent_i in doc.annotations:\n",
    "#             # print(sent_i, doc.annotations[sent_i])\n",
    "#             for layer_name in doc.annotations[sent_i]:\n",
    "#                 if layer_name == 'pain':\n",
    "#                     for span in doc.annotations[sent_i]['pain']:\n",
    "#                         print(span.props)\n",
    "                \n",
    "# #                 if layer_name == 'pain-at':\n",
    "# #                     print(doc.annotations[sent_i][layer_name])\n",
    "                    \n",
    "# #                 if layer_name == 'TIMEX3':\n",
    "# #                     for span in doc.annotations[sent_i][layer_name]:\n",
    "# #                         print(span.text, span.normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwe.utils import build_candidate_set\n",
    "\n",
    "Xs_pain_at = build_candidate_set(documents[0], \"pain-at\")\n",
    "#Xs_comp_at = build_candidate_set(documents[0], \"comp-at\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27718_11314_111136 4\n",
      "27162_29346_185828 41\n",
      "24347_63_195961 2\n",
      "27695_13917_192671 13\n",
      "22934_11162_113700 35\n",
      "271_61898_170625 19\n",
      "1843_57753_109617 24\n",
      "23500_7383_156659 11\n",
      "27673_26993_194796 4\n",
      "1880_13330_193396 19\n",
      "26845_31054_119104 13\n",
      "24498_57342_113484 28\n",
      "19430_7400_190280 13\n",
      "25243_3811_187890 12\n",
      "25035_51203_193364 17\n",
      "1711_67619_132244 11\n",
      "18971_19649_132894 6\n",
      "17242_9616_194603 5\n",
      "21790_18230_127941 4\n",
      "1885_24242_185235 9\n",
      "23714_4574_133306 4\n",
      "2604_68251_150426 16\n",
      "28492_21867_175298 10\n",
      "21984_21916_174115 8\n",
      "26559_19286_195185 12\n",
      "1757_29581_122726 9\n",
      "24237_23792_134550 5\n",
      "28875_7319_103943 11\n",
      "16932_67853_178133 7\n",
      "19420_91074_106110 34\n",
      "28895_10758_183338 6\n",
      "24867_2008_184705 6\n",
      "25496_25519_145528 10\n",
      "16877_22927_197951 9\n",
      "1946_88857_124316 24\n",
      "23025_29965_154849 8\n",
      "19532_44156_181689 6\n",
      "20705_15862_114219 3\n",
      "20086_16055_177220 4\n",
      "28381_68704_193105 6\n",
      "25944_78995_124007 7\n",
      "25018_7921_177363 2\n",
      "1679_6989_197945 10\n",
      "1960_59291_109058 8\n",
      "2364_44605_191210 7\n",
      "21850_21834_143794 4\n",
      "18751_8799_198888 15\n",
      "24274_9082_143003 8\n",
      "26190_18062_165800 6\n",
      "2322_12938_165691 4\n",
      "21786_20578_108226 6\n",
      "17310_88227_131508 4\n",
      "18669_95031_133784 7\n",
      "1725_90830_135095 2\n",
      "21609_24547_129957 2\n",
      "580\n"
     ]
    }
   ],
   "source": [
    "def collapse_relation_args(relations):\n",
    "    return set([s for x in relations for s in x])\n",
    "    \n",
    "# print some summary stats about candidates\n",
    "pain_at_spans = collapse_relation_args(Xs_pain_at)\n",
    "#comp_at_spans = collapse_relation_args(Xs_comp_at)\n",
    "\n",
    "doc_span_index = collections.defaultdict(set)\n",
    "#for s in comp_at_spans:\n",
    "#    doc_span_index[s.sentence.document.name].add(s)\n",
    "for s in pain_at_spans:\n",
    "    doc_span_index[s.sentence.document.name].add(s)\n",
    "    \n",
    "n = 0\n",
    "for doc_name in doc_span_index:\n",
    "    print(doc_name, len(doc_span_index[doc_name]))\n",
    "    n += len(doc_span_index[doc_name])\n",
    "print(n)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = '/Users/fries/Desktop/mimic-sample-brat/'\n",
    "etype = 'Concept'\n",
    "\n",
    "for doc_name in doc_span_index:\n",
    "    outfname = f'{outdir}/{doc_name}.ann'\n",
    "    with open(outfname, 'w') as fp:\n",
    "        items = set()\n",
    "        for i,s in enuamerate(doc_span_index[doc_name]):\n",
    "            # T8\tConcept 468 479;480 489\tright lower extremity\n",
    "            multi_spans = []\n",
    "            start = s.abs_char_start\n",
    "            if '\\n' in s.text:\n",
    "                toks = s.text.split(\"\\n\")\n",
    "                for t in toks:\n",
    "                    multi_spans.append((start, start + len(t)))\n",
    "                    start += len(t) + 1\n",
    "                \n",
    "                span_str = [f'{span[0]} {span[1]}' for span in multi_spans]\n",
    "                anno = (etype, \" \", \";\".join(span_str), \"\\t\", s.text.replace(\"\\n\", \" \"))\n",
    "            else:\n",
    "                anno = (etype, \" \", f\"{s.abs_char_start} {s.abs_char_end+1}\", \"\\t\", s.text.replace(\"\\n\", \" \"))\n",
    "                \n",
    "            items.add(anno)\n",
    "            \n",
    "        for i,s in enumerate(sorted(items,key=lambda x:x[1], reverse=0)):\n",
    "            anno = f'T{i+1}\\t{\"\".join(s)}'\n",
    "            print(anno)\n",
    "            fp.write(anno + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwe.utils import build_candidate_set, load_gold\n",
    "\n",
    "fpath = \"../data/annotations/mimic.gold.final.tsv\"\n",
    "gold = load_gold(fpath, documents[0], ('part-at', ('pain','anatomy')))\n",
    "\n",
    "Xs = build_candidate_set(documents[0], \"pain-at\")\n",
    "Ys = [gold[x] if x in gold else 0 for x in Xs]\n",
    "Ys = [y if y == 1 else 2 for y in Ys]\n",
    "\n",
    "print(\"Class Balance\")\n",
    "print(f'Positive: {Ys.count(1)} ({Ys.count(1)/len(Ys)*100:2.1f})%')\n",
    "print(f'Negative: {Ys.count(2)} ({Ys.count(2)/len(Ys)*100:2.1f})%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply Labeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from rwe.labelers.taggers import get_left_span, get_right_span, get_between_span\n",
    "\n",
    "def dict_matches(span, dictionary):\n",
    "    matches = []\n",
    "    toks = span.get_attrib_tokens('words')\n",
    "    for i in range(len(toks)):\n",
    "        for j in range(i+1, len(toks)):\n",
    "            term = ' '.join(toks[i:j]).lower()\n",
    "            if term in dictionary:\n",
    "                matches.append(term)\n",
    "    return matches\n",
    "\n",
    "ABSTAIN  = 0\n",
    "NEGATIVE = 2\n",
    "POSITIVE = 1\n",
    "\n",
    "neg_rgx = re.compile(\n",
    "    r'''\\b(insensitivity|paresthesias|paresthesia|sensitivity|tenderness|discomfort|heaviness|sensitive|itchiness|tightness|throbbing|numbness|tingling|cramping|coldness|soreness|painful|hurting|itching|burning|tender|buring|aching|hurts|aches|pains|hurt|pain|ache|achy|numb)\\b''',\n",
    "    re.I\n",
    ")\n",
    "\n",
    "def LF_is_negated(x):\n",
    "    return NEGATIVE if 'negated' in x.pain.props else ABSTAIN\n",
    "\n",
    "def LF_is_hypothetical(x):\n",
    "    return NEGATIVE if 'hypothetical' in x.pain.props else ABSTAIN\n",
    "\n",
    "def LF_section_headers(x):\n",
    "    sections = {\n",
    "        'past medical history': NEGATIVE,\n",
    "        'chief complaint': POSITIVE,\n",
    "        'discharge instructions': NEGATIVE,\n",
    "        'discharge condition': NEGATIVE\n",
    "    }\n",
    "    header = x.pain.props['section'].text.lower() if x.pain.props['section'] else None\n",
    "    return ABSTAIN if header not in sections else sections[header]\n",
    "\n",
    "def LF_contiguous_args(x):\n",
    "    v = not get_between_span(x.pain, x.anatomy)\n",
    "    v &= not 'negated' in x.pain.props\n",
    "    v &= not 'hypothetical' in x.pain.props\n",
    "    return POSITIVE if v else ABSTAIN\n",
    "\n",
    "def LF_distant_args(x, max_toks=10):\n",
    "    \"\"\"Reject candidate if the arguments occur too far apart (in token distance).\"\"\"\n",
    "    span = get_between_span(x.pain, x.anatomy)\n",
    "    n_toks = len(span.get_attrib_tokens('words')) if span else 0\n",
    "    return NEGATIVE if n_toks > max_toks else ABSTAIN\n",
    "    \n",
    "def LF_between_terms(x):\n",
    "    \"\"\"Reject if some key terms occur between arguments.\"\"\"\n",
    "    span = get_between_span(x.pain, x.anatomy)\n",
    "    if not span:\n",
    "        return ABSTAIN\n",
    "    # negation term      \n",
    "    flag = neg_rgx.search(span.text) is not None\n",
    "    # anatomical term \n",
    "    flag |= len(dict_matches(span, dict_anat)) > 0\n",
    "    return NEGATIVE if flag else ABSTAIN\n",
    "\n",
    "def LF_complains_of(x):\n",
    "    rgx = re.compile(r'''\\b(complain(s*|ing*) of)\\b''', re.I)\n",
    "    is_negated = 'negated' in x.pain.props\n",
    "    is_complains_of = rgx.search(get_left_span(x.pain).text, re.I) is not None\n",
    "    return POSITIVE if not is_negated and is_complains_of else ABSTAIN\n",
    "    \n",
    "\n",
    "lfs = [\n",
    "    LF_is_negated,\n",
    "    LF_is_hypothetical,\n",
    "    LF_section_headers,\n",
    "    LF_contiguous_args,\n",
    "    LF_distant_args,\n",
    "    LF_between_terms,\n",
    "    LF_complains_of\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "            \n",
    "\n",
    "# # for x in Xs:\n",
    "# #     v = LF_between_terms(x)\n",
    "# #     print(v)\n",
    "\n",
    "# x = Xs[10]\n",
    "# span = get_between_span(x.pain, x.anatomy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from rwe.labelers import LabelingServer\n",
    "\n",
    "labeler = LabelingServer(num_workers=4)\n",
    "Ls = labeler.apply(lfs, [Xs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwe.analysis import lf_summary\n",
    "\n",
    "lf_summary(Ls[0], Y=Ys, lf_names=[lf.__name__ for lf in lfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwe.visualization.analysis import view_conflicts, view_label_matrix, view_overlaps\n",
    "\n",
    "view_overlaps(Ls[0], normalize=False)\n",
    "view_label_matrix(Ls[0])\n",
    "view_conflicts(Ls[0], normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Snorkel Label Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sparse matrix to new Snorkel format\n",
    "\n",
    "def convert_label_matrix(L):\n",
    "    L = L.toarray().copy()\n",
    "    L[L == 0] = -1\n",
    "    L[L == 2] = 0\n",
    "    return L\n",
    "\n",
    "\n",
    "Ls_hat = [\n",
    "    convert_label_matrix(Ls[0])\n",
    "]\n",
    "\n",
    "Ys_hat = [\n",
    "    np.array([0 if y == 2 else 1 for y in Ys])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LabelModel\n",
    "\n",
    "lr = 0.01\n",
    "l2 = 0.001\n",
    "prec_init = 0.9\n",
    "\n",
    "label_model = LabelModel(cardinality=2, device='cpu', verbose=True)\n",
    "label_model.fit(L_train=Ls_hat[0], \n",
    "                n_epochs=1000, \n",
    "                lr=lr,\n",
    "                l2=l2,\n",
    "                prec_init=prec_init,\n",
    "                optimizer='adam',\n",
    "                log_freq=100)\n",
    "\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "label_model.score(L=Ls_hat[0], Y=Ys_hat[0], metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.7 (bert)",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
