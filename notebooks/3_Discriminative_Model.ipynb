{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Extract Pain Outcomes from Clinical Text without Labeled Data\n",
    "## II: Discriminative Model\n",
    "\n",
    "We show 2 standard discriminative model:\n",
    "\n",
    "- Bidirectional Long Short Term Memory (LSTM)\n",
    "- Sparse Logisitic Regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numba\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.learning.disc_models.rnn import *\n",
    "from snorkel.annotations import LabelAnnotator\n",
    "from snorkel.annotations import load_gold_labels\n",
    "from snorkel.models import candidate_subclass, Document, Sentence, Candidate, Span\n",
    "from snorkel.learning import GenerativeModel\n",
    "from rwe.extractlib.labelers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling Functions n=24\n"
     ]
    }
   ],
   "source": [
    "session = SnorkelSession()\n",
    "\n",
    "try:\n",
    "    PainLocation = candidate_subclass('PainLocation', ['pain', 'anatomy'])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "relation = PainLocationRelation(dict_root=\"../data/\")\n",
    "print \"Labeling Functions n={}\".format(len(relation.lfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Candidates and Gold Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold [TRAIN] 224\n",
      "Gold [DEV]   63\n",
      "Gold [TEST]  165\n"
     ]
    }
   ],
   "source": [
    "train_cands = session.query(Candidate).filter(Candidate.split == 0).order_by(Candidate.id).all()\n",
    "dev_cands   = session.query(Candidate).filter(Candidate.split == 1).order_by(Candidate.id).all()\n",
    "test_cands  = session.query(Candidate).filter(Candidate.split == 2).order_by(Candidate.id).all()\n",
    "\n",
    "L_gold_train = load_gold_labels(session, split=0, annotator_name='gold')\n",
    "L_gold_dev   = load_gold_labels(session, split=1, annotator_name='gold')\n",
    "L_gold_test  = load_gold_labels(session, split=2, annotator_name='gold')\n",
    "\n",
    "print \"Gold [TRAIN]\", L_gold_train.size\n",
    "print \"Gold [DEV]  \", L_gold_dev.size\n",
    "print \"Gold [TEST] \", L_gold_test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = session.query(Document).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225\n",
      "63\n",
      "168\n"
     ]
    }
   ],
   "source": [
    "print len(train_cands)\n",
    "print len(dev_cands)\n",
    "print len(test_cands)\n",
    "\n",
    "candidates = train_cands + dev_cands + test_cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225, 28)\n",
      "(63, 28)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.annotations import LabelAnnotator\n",
    "from rwe.extractlib.relations.anatomy_pain import *\n",
    "\n",
    "relations = AnatomyPainRelation(candidates, data_root=\"../data/\")\n",
    "lfs = relations.lfs\n",
    "\n",
    "labeler = LabelAnnotator(lfs=lfs)\n",
    "L_train = labeler.load_matrix(session, split=0)\n",
    "L_dev   = labeler.load_matrix(session, split=1)\n",
    "\n",
    "print L_train.shape\n",
    "print L_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sparse Logistic Regression\n",
    "\n",
    "### Create Features\n",
    "This uses a standard NLP feature generation library using lemmatization, POS tags, and sentence dependency parsing to generate candidate features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from snorkel.annotations import FeatureAnnotator\n",
    "from rwe.extractlib.features import hybrid_span_mention_ftrs\n",
    "\n",
    "featurizer = FeatureAnnotator(hybrid_span_mention_ftrs)\n",
    "\n",
    "F_train = featurizer.load_matrix(session, split=0)\n",
    "F_dev   = featurizer.load_matrix(session, split=1)\n",
    "F_test  = featurizer.load_matrix(session, split=2)\n",
    "\n",
    "F_train = F_train if F_train.size != 0 else featurizer.apply(split=0)\n",
    "F_dev   = F_dev if F_dev.size != 0 else featurizer.apply_existing(split=1)\n",
    "F_test  = F_test if F_test.size != 0 else featurizer.apply_existing(split=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,recall_score,precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "from snorkel.learning.utils import MentionScorer\n",
    "from snorkel.learning import RandomSearch, ListParameter, RangeParameter\n",
    "\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Discriminitive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.learning import SparseLogisticRegression, LogisticRegression\n",
    "disc_model = SparseLogisticRegression()\n",
    "#disc_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "use_supervised = False\n",
    "t_marginals = gold_train_marginals if use_supervised else train_marginals\n",
    "    \n",
    "rate_param = RangeParameter('lr', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l1_param   = RangeParameter('l1_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "l2_param   = RangeParameter('l2_penalty', 1e-6, 1e-2, step=1, log_base=10)\n",
    "\n",
    "param_grid = [rate_param, l1_param, l2_param]\n",
    "\n",
    "searcher = RandomSearch(session, disc_model, F_train, t_marginals,\n",
    "                        param_grid, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(123456)\n",
    "searcher.fit(F_dev, L_gold_dev, n_epochs=2000, rebalance=0.5, print_freq=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models import GoldLabel, GoldLabelKey, Label, LabelKey, Feature, FeatureKey, Candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = disc_model.error_analysis(session, F_test, L_gold_test, b=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = disc_model.marginals(F_test)\n",
    "fig, ax = plt.subplots()\n",
    "df = pd.DataFrame(data=m, columns=['marginals'])\n",
    "pd.DataFrame.hist(df,range=(0.0, 1.0),bins=20, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_marginals(cands, marginals):\n",
    "    rows = [\"\\t\".join([\"DOC_NAME\",\"CID\",\"SID\",\"PAIN\",\"ANATOMY\",\"MARGINAL\"])]\n",
    "    for i,c in enumerate(cands):\n",
    "        row = [c.get_parent().document.name, c.id, c.get_parent().id]\n",
    "        row += [c.pain.get_span(), c.anatomy.get_span()]\n",
    "        row += [marginals[i]]\n",
    "        rows.append(\"\\t\".join(map(str,row)))\n",
    "    return \"\\n\".join(rows)\n",
    "\n",
    "test_marginals = disc_model.marginals(F_test)\n",
    "#dump_marginals(test_cands, test_marginals)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# supervised n=25 random search\n",
    "========================================\n",
    "Scores (Un-adjusted)\n",
    "========================================\n",
    "Pos. class accuracy: 0.867\n",
    "Neg. class accuracy: 0.81\n",
    "Precision            0.677\n",
    "Recall               0.867\n",
    "F1                   0.76\n",
    "----------------------------------------\n",
    "TP: 65 | FP: 31 | TN: 132 | FN: 10\n",
    "========================================\n",
    "\n",
    "========================================\n",
    "Scores (Un-adjusted)\n",
    "========================================\n",
    "Pos. class accuracy: 0.76\n",
    "Neg. class accuracy: 0.64\n",
    "Precision            0.514\n",
    "Recall               0.76\n",
    "F1                   0.613\n",
    "----------------------------------------\n",
    "TP: 19 | FP: 18 | TN: 32 | FN: 6\n",
    "========================================\n",
    "\n",
    "\n",
    "# weak\n",
    "========================================\n",
    "Scores (Un-adjusted)\n",
    "========================================\n",
    "Pos. class accuracy: 0.76\n",
    "Neg. class accuracy: 0.89\n",
    "Precision            0.76\n",
    "Recall               0.76\n",
    "F1                   0.76\n",
    "----------------------------------------\n",
    "TP: 57 | FP: 18 | TN: 145 | FN: 18\n",
    "========================================\n",
    "\n",
    "========================================\n",
    "Scores (Un-adjusted)\n",
    "========================================\n",
    "Pos. class accuracy: 0.92\n",
    "Neg. class accuracy: 0.68\n",
    "Precision            0.59\n",
    "Recall               0.92\n",
    "F1                   0.719\n",
    "----------------------------------------\n",
    "TP: 23 | FP: 16 | TN: 34 | FN: 2\n",
    "========================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L_train.lf_stats(session, labels=L_gold_train.toarray().ravel())\n",
    "L_dev.lf_stats(session, labels=L_gold_dev.toarray().ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "Long Short Term Memory (LSTM) models can acheive state-of-the-art performance on many text classification tasks. We'll train a simple bidirectional LSTM model below.\n",
    "\n",
    "In deep learning, hyperparameter tuning is very important and computationally expensive step in training models. For purposes of this tutorial, we've pre-selected some settings so that you can train a model in under 10 minutes. Advanced users can look at our Grid Search Tutorial for more details on choosing these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_train_marginals = np.array([1 if train_marginals[i] > 0.5 else 0 for i in range(len(train_marginals))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_train_marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def get_max_seq_len(cands):\n",
    "    l = 0\n",
    "    for c in cands:\n",
    "        l = max(len(c[0].sentence.words),l)\n",
    "    print \"max seq len\", l\n",
    "    return l\n",
    "\n",
    "attn_window      = ListParameter('attn_window', [0]) # get_max_seq_len(dev_cands)\n",
    "batch_size_param = ListParameter('batch_size', [32, 64])\n",
    "rate_param       = RangeParameter('lr', 1e-4, 1e-2, step=1, log_base=10)\n",
    "dropout_param    = RangeParameter('dropout', 0.0, 0.5, step=0.25)\n",
    "dim_param        = ListParameter('dim', [50, 100])\n",
    "\n",
    "param_grid = [attn_window, rate_param, dropout_param, dim_param, batch_size_param]\n",
    "\n",
    "lstm = reRNN()\n",
    "searcher = RandomSearch(session, lstm, train_cands, train_marginals, param_grid, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher.fit(dev_cands, L_gold_dev, n_epochs=400, rebalance=0.0, print_freq=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = lstm.marginals(test_cands)\n",
    "fig, ax = plt.subplots()\n",
    "df = pd.DataFrame(data=m, columns=['marginals'])\n",
    "pd.DataFrame.hist(df,range=(0.0, 1.0),bins=20, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = lstm.error_analysis(session, test_cands, L_gold_test, b=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = list(L_gold_test).count(1)\n",
    "neg = list(L_gold_test).count(-1)\n",
    "\n",
    "print pos/(float(neg)+float(pos))\n",
    "\n",
    "import sys\n",
    "def save_model(model, out_dir):\n",
    "    if os.path.exists(out_dir):\n",
    "        print>>sys.stderr,\"warning, model already exists\"\n",
    "    else:\n",
    "        os.mkdir(out_dir)\n",
    "        \n",
    "    model.save(out_dir+\"/model\")\n",
    "\n",
    "save_model(lstm,\"/users/fries/desktop/foobar/\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hard Labels\n",
    "========================================\n",
    "Scores (Un-adjusted)\n",
    "========================================\n",
    "Pos. class accuracy: 0.725\n",
    "Neg. class accuracy: 0.847\n",
    "Precision            0.659\n",
    "Recall               0.725\n",
    "F1                   0.69\n",
    "----------------------------------------\n",
    "TP: 29 | FP: 15 | TN: 83 | FN: 11\n",
    "========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = lstm.marginals(test_cands)\n",
    "fig, ax = plt.subplots()\n",
    "df = pd.DataFrame(data=m, columns=['marginals'])\n",
    "pd.DataFrame.hist(df,range=(0.0, 1.0),bins=20, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_gold_test = load_gold_labels(session, annotator_name='gold', cand_gen=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = disc_model.error_analysis(session, F_test, L_gold_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# no dependencies\n",
    "========================================\n",
    "Scores (Un-adjusted)\n",
    "========================================\n",
    "Pos. class accuracy: 0.739\n",
    "Neg. class accuracy: 0.673\n",
    "Precision            0.5\n",
    "Recall               0.739\n",
    "F1                   0.596\n",
    "----------------------------------------\n",
    "TP: 17 | FP: 17 | TN: 35 | FN: 6\n",
    "========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
