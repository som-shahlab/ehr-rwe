{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Learning to Extract Pain Outcomes from Clinical Text without Labeled Data\n",
    "## I: Preprocessing Clinical Notes\n",
    "\n",
    "This pipeline extracts present positive indications of pain and its anatomical location from patient clinical notes. To train our machine reading models, we use [*weak supervision*](https://hazyresearch.github.io/snorkel/blog/weak_supervision.html), a technique that enables training deep learning models using large collections of unlabeled clinical documents. All weak supervision models are trained using [Snorkel](https://hazyresearch.github.io/snorkel/).\n",
    "\n",
    "This demo uses notes from [MIMIC-III](https://mimic.physionet.org/), a collection of electronic health record (EHR) data for ~40,000 critical care patients. For validating extraction performance, we manually annotated a subset of MIMIC clinical notes released as part of the [ShARe/CLEF eHealth Evaluation Lab 2014](https://link.springer.com/chapter/10.1007/978-3-319-11382-1_17).\n",
    "\n",
    "<img align=\"center\" src=\"pain-anatomy-relations.jpg\" width=\"650px\" style=\"border:1px solid black; margin-bottom:15px\">\n",
    "**Figure 1** *Example pain/anatomy mention types. This demo shows a binary classifier for discrimitating present positive mentions from negated, hypothetical, and historical mentions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import bz2\n",
    "import glob\n",
    "import codecs\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "\n",
    "from rwe.utils import *\n",
    "from rwe.corpora import ClefCorpus\n",
    "\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Parse Document Collection\n",
    "All clinical documents live in the `../data/corpora/MIMIC-III/training/` directory.\n",
    "We preprocess these documents using off-the-shelf natural language processing (NLP) tools to generate a collection of tokenized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from snorkel.models import Document, Sentence, Candidate\n",
    "from snorkel.parser import Spacy, RegexTokenizer, RuleBasedParser\n",
    "from snorkel.parser import CorpusParser, CorpusParserUDF, StanfordCoreNLPServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory ../data/corpora/clef/2014ShAReCLEFeHealthTasks2/cache\n",
      "299\n"
     ]
    }
   ],
   "source": [
    "from snorkel.parser import TextDocPreprocessor\n",
    "\n",
    "doc_root = \"../data/corpora/clef/2014ShAReCLEFeHealthTasks2/\"\n",
    "corpus   = ClefCorpus(doc_root + \"training/\")\n",
    "\n",
    "filelist = \"{}/*.txt\".format(corpus.cachedir)\n",
    "\n",
    "doc_preprocessor = TextDocPreprocessor(path=filelist, encoding=corpus.encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Selecting a Parser\n",
    "You can use [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/) or [spaCy](https://spacy.io/) for preprocessing. In some datasets, CoreNLP is more robust for clinical text sentence boundary detection, though significantly slower than spaCy. You may also specify a combination of regular expression / spaCy for finer control of sentence boundary detection and tokenization.\n",
    "\n",
    "### MIMIC-II/CLEF 2014 Corpus Summary\n",
    "Documents: 299\n",
    "Sentences: 16164\n",
    "\n",
    "### Parse Time Performance Comparisons\n",
    "\n",
    "| Parser                 | Wall Time (secs) |\n",
    "|------------------------|------------------|\n",
    "| spaCy                  | 17.5             |\n",
    "| CoreNLP                | 64.0             |\n",
    "| RuleBased (spaCy+regex) | 12.3             |\n",
    "| RuleBased (regex+regex) | 11.3             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# custom CoreNLP configuration\n",
    "annotator_opts = {}\n",
    "annotator_opts['ssplit']   = {\"newlineIsSentenceBreak\": \"two\"}\n",
    "annotator_opts['tokenize'] = {\"invertible\": True,\n",
    "                              \"normalizeFractions\": False,\n",
    "                              \"normalizeParentheses\": False,\n",
    "                              \"normalizeOtherBrackets\": False,\n",
    "                              \"normalizeCurrency\": False,\n",
    "                              \"asciiQuotes\": False,\n",
    "                              \"latexQuotes\": False,\n",
    "                              \"ptb3Ellipsis\": False,\n",
    "                              \"ptb3Dashes\": False,\n",
    "                              \"escapeForwardSlashAsterisk\": False,\n",
    "                              \"strictTreebank3\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "PARSER = \"corenlp\"\n",
    "\n",
    "if PARSER == \"corenlp\":\n",
    "    parser = StanfordCoreNLPServer(annotator_opts=annotator_opts, \n",
    "                                   verbose=False, version='3.6.0',\n",
    "                                   split_newline=False,\n",
    "                                   num_threads=4)\n",
    "elif PARSER == \"rgx\":\n",
    "    parser = RuleBasedParser(tokenizer=RegexTokenizer())\n",
    "\n",
    "else:\n",
    "    parser = Spacy(lang=\"en\")\n",
    "\n",
    "corpus_parser = CorpusParser(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 16.6 s, sys: 391 ms, total: 16.9 s\n",
      "Wall time: 3min 17s\n"
     ]
    }
   ],
   "source": [
    "# sqlite3 database instances do not support parallelism > 1 \n",
    "%time corpus_parser.apply(doc_preprocessor, parallelism=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 299\n",
      "Sentences: 16164\n"
     ]
    }
   ],
   "source": [
    "print \"Documents:\", session.query(Document).count()\n",
    "print \"Sentences:\", session.query(Sentence).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Candidate Extraction\n",
    "\n",
    "### Create Matchers\n",
    "\n",
    "Candidates are *possible* positive instances of a pain outcome. They are defined as the Cartesian product of all `(pain, anatomy)` entity pairs found in a given sentence. Generating these pairs requires creating 2 `Matcher` objects, which use dictionary string matching and (optional) regular expressions to identify entities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import *\n",
    "from rwe.matchers import *\n",
    "\n",
    "dict_root       = \"../data/supervision/dicts/\"\n",
    "dict_anatomy    = load_dict(\"{}anatomy/fma_human_anatomy.bz2\".format(dict_root))\n",
    "anatomy_matcher = AnatomicalSiteMatcher(dict_anatomy)\n",
    "\n",
    "dict_pain       = load_dict(\"{}nociception/nociception.curated.txt\".format(dict_root))\n",
    "pain_matcher    = PainMatcher(dict_pain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define Candidate Schema\n",
    "This defines a relation type that Snorkel uses behind the scenes to generate candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "try:\n",
    "    PainLocation = candidate_subclass('PainLocation', ['pain','anatomy'])\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Extract Candidate Pain/Anatomy Relations\n",
    "\n",
    "The `Ngrams` object defines the maximum token length for matching entities. \n",
    "\n",
    "Using CoreNLP for parsing, you should get:\n",
    "\n",
    "```\n",
    "Development Set Candidates: 288\n",
    "Test Set Candidates:        168\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "\n",
    "ngrams = Ngrams(n_max=5, split_tokens=[\"-\",\"/\",\":\",\";\",\",\"])\n",
    "\n",
    "cand_extractor = CandidateExtractor(PainLocation, \n",
    "                                    [ngrams, ngrams], [pain_matcher, anatomy_matcher],\n",
    "                                    symmetric_relations=True, nested_relations=False, \n",
    "                                    self_relations=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from snorkel.models import Document\n",
    "from rwe.utils import split_training_test_dev\n",
    "\n",
    "def filter_by_sentence_length(docs, max_length=85):\n",
    "    '''\n",
    "    Prevent pathological case where our sentence explodes the number\n",
    "    of candidate relations.\n",
    "    This happens offen with incorrectly parsed clinical text\n",
    "\n",
    "    :param docs:\n",
    "    :param max_length:\n",
    "    :return:\n",
    "    '''\n",
    "    for i,doc in enumerate(docs):\n",
    "        for s in doc.sentences:\n",
    "            if len(s.words) <= max_length:\n",
    "                yield (s,doc.stable_id)\n",
    "                \n",
    "docs = session.query(Document).all()\n",
    "sentences = {fold:set() for fold in set(corpus.fold_idx.values())}\n",
    "\n",
    "for s,doc_id in filter_by_sentence_length(docs):\n",
    "    fold = corpus.fold_idx[doc_id]\n",
    "    sentences[fold].add(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 53.3 s, sys: 612 ms, total: 53.9 s\n",
      "Wall time: 53.8 s\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 20.8 s, sys: 456 ms, total: 21.3 s\n",
      "Wall time: 21.2 s\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 42.4 s, sys: 680 ms, total: 43.1 s\n",
      "Wall time: 43.3 s\n"
     ]
    }
   ],
   "source": [
    "for i,fold in enumerate(sentences):\n",
    "    %%time cand_extractor.apply(sentences[fold], split=i, parallelism=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 225\n",
      "Number of candidates: 63\n",
      "Number of candidates: 168\n",
      "456\n"
     ]
    }
   ],
   "source": [
    "train_cands = session.query(PainLocation).filter(PainLocation.split == 0).all()\n",
    "print \"Number of candidates:\", len(train_cands)\n",
    "\n",
    "dev_cands = session.query(PainLocation).filter(PainLocation.split == 1).all()\n",
    "print \"Number of candidates:\", len(dev_cands)\n",
    "\n",
    "test_cands = session.query(PainLocation).filter(PainLocation.split == 2).all()\n",
    "print \"Number of candidates:\", len(test_cands)\n",
    "\n",
    "print len(train_cands) + len(dev_cands) + len(test_cands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Loading Gold Labels\n",
    "Weakly supervised models require a small amount of hand labeled data in order to tune the hyperparamters of model and validate end model performance. \n",
    "\n",
    "Gold labels can be generated using the [BRAT](http://brat.nlplab.org/) annotation tool or Snorkel's internal candidate annotator (see supplimental notebooks).\n",
    "\n",
    "We've provided annotations for 452 pain/anatomy candidate mentions using MIMIC documents released as part of the [ShARe/CLEF eHealth Evaluation Lab 2014](https://link.springer.com/chapter/10.1007/978-3-319-11382-1_17).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File ../data/annotations/clef.gold.2017.5.22.tsv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4d2ee5e1a02e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mgold_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_fpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mload_external_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotator_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fries/miniconda2/envs/py27/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fries/miniconda2/envs/py27/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fries/miniconda2/envs/py27/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fries/miniconda2/envs/py27/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/fries/miniconda2/envs/py27/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4184)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8449)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File ../data/annotations/clef.gold.2017.5.22.tsv does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from snorkel.models import StableLabel\n",
    "from snorkel.annotations import load_gold_labels\n",
    "from snorkel.db_helpers import reload_annotator_labels\n",
    "\n",
    "gold_fpath = \"../data/annotations/clef.gold.final.tsv\"\n",
    "\n",
    "def load_external_labels(session, df, annotator_name):\n",
    "    \"\"\"\n",
    "    Load pandas dataframe of label annotations. These are imported as StableLabel\n",
    "    objects are used to create labeled gold candidates\n",
    "\n",
    "    :param session:\n",
    "    :param df:  pandas data frame containing labels\n",
    "    :param annotator_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for index, row in df.iterrows():\n",
    "        # We check if the label already exists, in case this cell was already executed\n",
    "        context_stable_ids = row['context_stable_ids'] if 'context_stable_ids' in row else \"~~\".join([row['pain'], row['anatomy']])\n",
    "        query = session.query(StableLabel).filter(StableLabel.context_stable_ids == context_stable_ids)\n",
    "        query = query.filter(StableLabel.annotator_name == annotator_name)\n",
    "        if query.count() == 0:\n",
    "            session.add(StableLabel(context_stable_ids=context_stable_ids, annotator_name=annotator_name, value=row['label']))\n",
    "    session.commit()\n",
    "\n",
    "    \n",
    "gold_labels = pd.read_csv(gold_fpath, sep=\"\\t\")\n",
    "load_external_labels(session, gold_labels, annotator_name=\"gold\")\n",
    "\n",
    "reload_annotator_labels(session, PainLocation, \"gold\", split=0, \n",
    "                        filter_label_split=False, create_missing_cands=False)\n",
    "reload_annotator_labels(session, PainLocation, \"gold\", split=1, \n",
    "                        filter_label_split=False, create_missing_cands=False)\n",
    "reload_annotator_labels(session, PainLocation, \"gold\", split=2, \n",
    "                        filter_label_split=False, create_missing_cands=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
