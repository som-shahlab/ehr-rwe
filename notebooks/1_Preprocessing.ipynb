{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Extract Pain Outcomes from Clinical Text without Labeled Data\n",
    "## I: Preprocessing Clinical Notes\n",
    "\n",
    "This pipeline extracts present positive indications of pain and its anatomical location from patient clinical notes. To train our machine reading models, we use [*weak supervision*](https://hazyresearch.github.io/snorkel/blog/weak_supervision.html), a technique that enables training deep learning models using large collections of unlabeled clinical documents. All weak supervision models are trained using [Snorkel](https://hazyresearch.github.io/snorkel/).\n",
    "\n",
    "This demo uses notes from [MIMIC-III](https://mimic.physionet.org/), a collection of electronic health record (EHR) data for ~40,000 critical care patients. For validating extraction performance, we manually annotated a subset of MIMIC clinical notes released as part of the [ShARe/CLEF eHealth Evaluation Lab 2014](https://link.springer.com/chapter/10.1007/978-3-319-11382-1_17).\n",
    "\n",
    "<img align=\"center\" src=\"pain-anatomy-relations.jpg\" width=\"650px\" style=\"border:1px solid black; margin-bottom:15px\">\n",
    "**Figure 1** Example pain/anatomy mention types. This demo shows a binary classifier for discrimitating present positive mentions from negated, hypothetical, and historical mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import bz2\n",
    "import glob\n",
    "import codecs\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "\n",
    "from rwe.extractlib.utils import *\n",
    "from rwe.extractlib.corpora import ClefCorpus\n",
    "\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parse Document Collection\n",
    "All clinical documents live in the `../data/corpora/MIMIC-III/training/` directory.\n",
    "We preprocess these documents using off-the-shelf natural language processing (NLP) tools to generate a collection of tokenized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models import Document, Sentence, Candidate\n",
    "from snorkel.parser import Spacy, RegexTokenizer, RuleBasedParser\n",
    "from snorkel.parser import CorpusParser, CorpusParserUDF, StanfordCoreNLPServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.parser import TextDocPreprocessor\n",
    "\n",
    "doc_root = \"../data/corpora/clef/2014ShAReCLEFeHealthTasks2/\"\n",
    "corpus   = ClefCorpus(doc_root + \"training/\")\n",
    "\n",
    "filelist = \"{}/*.txt\".format(corpus.cachedir)\n",
    "\n",
    "doc_preprocessor = TextDocPreprocessor(path=filelist, encoding=corpus.encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a Parser\n",
    "You can use [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/) or [spaCy](https://spacy.io/) for preprocessing. In some datasets, CoreNLP is more robust for clinical text sentence boundary detection, though significantly slower than spaCy. You may also specify a combination of regular expression / spaCy for finer control of sentence boundary detection and tokenization.\n",
    "\n",
    "### MIMIC-II/CLEF 2014 Corpus Summary\n",
    "Documents: 299\n",
    "Sentences: 16164\n",
    "\n",
    "### Parse Time Performance Comparisons\n",
    "\n",
    "| Parser                 | Wall Time (secs) |\n",
    "|------------------------|------------------|\n",
    "| spaCy                  | 17.5             |\n",
    "| CoreNLP                | 64.0             |\n",
    "| RuleBased (spaCy+regex) | 12.3             |\n",
    "| RuleBased (regex+regex) | 11.3             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom CoreNLP configuration\n",
    "annotator_opts = {}\n",
    "annotator_opts['ssplit']   = {\"newlineIsSentenceBreak\": \"two\"}\n",
    "annotator_opts['tokenize'] = {\"invertible\": True,\n",
    "                              \"normalizeFractions\": False,\n",
    "                              \"normalizeParentheses\": False,\n",
    "                              \"normalizeOtherBrackets\": False,\n",
    "                              \"normalizeCurrency\": False,\n",
    "                              \"asciiQuotes\": False,\n",
    "                              \"latexQuotes\": False,\n",
    "                              \"ptb3Ellipsis\": False,\n",
    "                              \"ptb3Dashes\": False,\n",
    "                              \"escapeForwardSlashAsterisk\": False,\n",
    "                              \"strictTreebank3\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PARSER = \"corenlp\"\n",
    "\n",
    "if PARSER == \"corenlp\":\n",
    "    parser = StanfordCoreNLPServer(annotator_opts=annotator_opts, \n",
    "                                   verbose=False, version='3.6.0',\n",
    "                                   split_newline=False,\n",
    "                                   num_threads=4)\n",
    "elif PARSER == \"rgx\":\n",
    "    parser = RuleBasedParser(tokenizer=RegexTokenizer())\n",
    "\n",
    "else:\n",
    "    parser = Spacy(lang=\"en\")\n",
    "\n",
    "corpus_parser = CorpusParser(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqlite3 database instances do not support parallelism > 1 \n",
    "%time corpus_parser.apply(doc_preprocessor, parallelism=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Documents:\", session.query(Document).count()\n",
    "print \"Sentences:\", session.query(Sentence).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Candidate Extraction\n",
    "\n",
    "### Create Matchers\n",
    "\n",
    "Candidates are *possible* positive instances of a pain outcome. They are defined as the Cartesian product of all `(pain, anatomy)` entity pairs found in a given sentence. Generating these pairs requires creating 2 `Matcher` objects, which use dictionary string matching and (optional) regular expressions to identify entities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.matchers import *\n",
    "from rwe.extractlib.custom_matchers import *\n",
    "\n",
    "dict_root       = \"../data/supervision/dicts/\"\n",
    "dict_anatomy    = load_dict(\"{}anatomy/fma_human_anatomy.bz2\".format(dict_root))\n",
    "anatomy_matcher = AnatomicalSiteMatcher(dict_anatomy)\n",
    "\n",
    "dict_pain       = load_dict(\"{}nociception/nociception.curated.txt\".format(dict_root))\n",
    "pain_matcher    = PainMatcher(dict_pain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Candidate Schema\n",
    "This defines a relation type that Snorkel uses behind the scenes to generate candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "try:\n",
    "    PainLocation = candidate_subclass('PainLocation', ['pain','anatomy'])\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Candidate Pain/Anatomy Relations\n",
    "\n",
    "The `Ngrams` object defines the maximum token length for matching entities. \n",
    "\n",
    "Using CoreNLP for parsing, you should get:\n",
    "\n",
    "```\n",
    "Development Set Candidates: 288\n",
    "Test Set Candidates:        168\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "\n",
    "ngrams = Ngrams(n_max=5, split_tokens=[\"-\",\"/\",\":\",\";\",\",\"])\n",
    "\n",
    "cand_extractor = CandidateExtractor(PainLocation, \n",
    "                                    [ngrams, ngrams], [pain_matcher, anatomy_matcher],\n",
    "                                    symmetric_relations=True, nested_relations=False, \n",
    "                                    self_relations=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.models import Document\n",
    "from rwe.extractlib.utils import split_training_test_dev\n",
    "\n",
    "def filter_by_sentence_length(docs, max_length=85):\n",
    "    '''\n",
    "    Prevent pathological case where our sentence explodes the number\n",
    "    of candidate relations.\n",
    "    This happens offen with incorrectly parsed clinical text\n",
    "\n",
    "    :param docs:\n",
    "    :param max_length:\n",
    "    :return:\n",
    "    '''\n",
    "    for i,doc in enumerate(docs):\n",
    "        for s in doc.sentences:\n",
    "            if len(s.words) <= max_length:\n",
    "                yield (s,doc.stable_id)\n",
    "                \n",
    "docs = session.query(Document).all()\n",
    "sentences = {fold:set() for fold in set(corpus.fold_idx.values())}\n",
    "\n",
    "for s,doc_id in filter_by_sentence_length(docs):\n",
    "    fold = corpus.fold_idx[doc_id]\n",
    "    sentences[fold].add(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,fold in enumerate(sentences):\n",
    "    %%time cand_extractor.apply(sentences[fold], split=i, parallelism=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cands = session.query(PainLocation).filter(PainLocation.split == 0).all()\n",
    "print \"Number of candidates:\", len(train_cands)\n",
    "\n",
    "dev_cands = session.query(PainLocation).filter(PainLocation.split == 1).all()\n",
    "print \"Number of candidates:\", len(dev_cands)\n",
    "\n",
    "test_cands = session.query(PainLocation).filter(PainLocation.split == 2).all()\n",
    "print \"Number of candidates:\", len(test_cands)\n",
    "\n",
    "print len(train_cands) + len(dev_cands) + len(test_cands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading Gold Labels\n",
    "Weakly supervised models require a small amount of hand labeled data in order to tune the hyperparamters of model and validate end model performance. \n",
    "\n",
    "Gold labels can be generated using the [BRAT](http://brat.nlplab.org/) annotation tool or Snorkel's internal candidate annotator (see supplimental notebooks).\n",
    "\n",
    "We've provided annotations for 452 pain/anatomy candidate mentions using MIMIC documents released as part of the [ShARe/CLEF eHealth Evaluation Lab 2014](https://link.springer.com/chapter/10.1007/978-3-319-11382-1_17).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from snorkel.models import StableLabel\n",
    "from snorkel.annotations import load_gold_labels\n",
    "from snorkel.db_helpers import reload_annotator_labels\n",
    "\n",
    "gold_fpath = \"../data/annotations/clef.gold.2017.5.22.tsv\"\n",
    "\n",
    "def load_external_labels(session, df, annotator_name):\n",
    "    \"\"\"\n",
    "    Load pandas dataframe of label annotations. These are imported as StableLabel\n",
    "    objects are used to create labeled gold candidates\n",
    "\n",
    "    :param session:\n",
    "    :param df:  pandas data frame containing labels\n",
    "    :param annotator_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for index, row in df.iterrows():\n",
    "        # We check if the label already exists, in case this cell was already executed\n",
    "        context_stable_ids = row['context_stable_ids'] if 'context_stable_ids' in row else \"~~\".join([row['pain'], row['anatomy']])\n",
    "        query = session.query(StableLabel).filter(StableLabel.context_stable_ids == context_stable_ids)\n",
    "        query = query.filter(StableLabel.annotator_name == annotator_name)\n",
    "        if query.count() == 0:\n",
    "            session.add(StableLabel(context_stable_ids=context_stable_ids, annotator_name=annotator_name, value=row['label']))\n",
    "    session.commit()\n",
    "\n",
    "    \n",
    "gold_labels = pd.read_csv(gold_fpath, sep=\"\\t\")\n",
    "load_external_labels(session, gold_labels, annotator_name=\"gold\")\n",
    "\n",
    "reload_annotator_labels(session, PainLocation, \"gold\", split=0, \n",
    "                        filter_label_split=False, create_missing_cands=False)\n",
    "reload_annotator_labels(session, PainLocation, \"gold\", split=1, \n",
    "                        filter_label_split=False, create_missing_cands=False)\n",
    "reload_annotator_labels(session, PainLocation, \"gold\", split=2, \n",
    "                        filter_label_split=False, create_missing_cands=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
