{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implant Complications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numba\n",
    "import metal\n",
    "import random\n",
    "from brat import *\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['SNORKELDB'] = \"postgresql://inkfish@127.0.0.1:4554/inkfish\"\n",
    "from snorkel import SnorkelSession\n",
    "\n",
    "#from snorkel.learning.disc_models.rnn import *\n",
    "from snorkel.annotations import LabelAnnotator\n",
    "from snorkel.annotations import load_gold_labels\n",
    "from snorkel.models import candidate_subclass, Document, Sentence, Candidate, Span\n",
    "from snorkel.learning import GenerativeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = SnorkelSession()\n",
    "\n",
    "# Define a candidate space\n",
    "try:\n",
    "    ImplantComplication = candidate_subclass('ImplantComplication', ['implant','complication'])\n",
    "except:\n",
    "    print(\"candidate subclass already exists, skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Candidates and Gold Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_doc_ids_by_name(session, doc_names):\n",
    "    return session.query(Document.id).filter(Document.name.in_(doc_names)).all()\n",
    "\n",
    "def get_cands_by_doc(session, doc_ids, candidate_class):\n",
    "    \"\"\"\n",
    "    :param session:\n",
    "    :param doc_ids:\n",
    "    :param candidate_class:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    q1 = session.query(Sentence.id).filter(Sentence.document_id.in_(doc_ids)).subquery()\n",
    "    q2 = session.query(Span.id).filter(Span.sentence_id.in_(q1)).subquery()\n",
    "    q3 = session.query(candidate_class.id).filter(candidate_class.implant_id.in_(q2)).subquery()\n",
    "    return session.query(Candidate).filter(Candidate.id.in_(q3)).all()\n",
    "\n",
    "def init_implant_complications_splits(session, num_training_docs=600):\n",
    "    \"\"\"\n",
    "    Create Train/Dev/Test splits\n",
    "    \"\"\"\n",
    "    np.random.seed(1234)\n",
    "\n",
    "    gold_docs = {'13899510', '23094601', '12189519', '20555550', '15347915', '15727234', '19325557', '13753588', \n",
    "                 '22090431', '14292702', '13881034', '19439270', '15147663', '10491490', '19655487', '11778639', \n",
    "                 '13612323', '19848445', '14718096', '10403411', '20195826', '11486237', '17576952', '15490123', \n",
    "                 '17579845', '1297680', '17410315', '18989394', '1012657', '19147933', '20860485', '12813059', \n",
    "                 '20968892', '10593203', '23348587', '1606569', '1006811', '22920273', '14116822', '10734397', \n",
    "                 '13645052', '12450938', '18029864', '15241311', '15854097', '21074206', '16490964', '1714600',\n",
    "                 '2368721', '23511775'}\n",
    "\n",
    "    gold_doc_ids = [id_[0] for id_ in get_doc_ids_by_name(session, gold_docs)]\n",
    "    \n",
    "    # build training set\n",
    "    doc_ids = [doc.id for doc in session.query(Document).all()]\n",
    "    train_doc_ids = random.sample(doc_ids, num_training_docs + len(gold_doc_ids))\n",
    "    \n",
    "    # ensure there isn't any overlap between training docs and dev/test\n",
    "    train_doc_ids = list(set(train_doc_ids).difference(gold_doc_ids))[0:num_training_docs]\n",
    "    print(\"Training docs: {}\".format(len(train_doc_ids)))\n",
    "\n",
    "    # train_doc_ids = set(train_doc_ids).difference(gold_docs)\n",
    "    train_cands = get_cands_by_doc(session, train_doc_ids , ImplantComplication)\n",
    "    \n",
    "    # split into dev/test splits\n",
    "    split = int(len(gold_doc_ids) * 0.50)\n",
    "    dev_cands   = get_cands_by_doc(session, gold_doc_ids[split:], ImplantComplication)\n",
    "    test_cands  = get_cands_by_doc(session, gold_doc_ids[0:split], ImplantComplication)\n",
    "\n",
    "    # assign to splits\n",
    "    for c in train_cands:\n",
    "        c.split = 1\n",
    "    for c in dev_cands:\n",
    "        c.split = 2\n",
    "    for c in test_cands:\n",
    "        c.split = 3\n",
    "        \n",
    "    session.commit()\n",
    "    \n",
    "def reset_split(candidates):\n",
    "    \"\"\"Remove all candidate split information.\"\"\"\n",
    "    for c in candidates:\n",
    "        c.split = 0\n",
    "    session.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load candidates and gold labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#init_implant_complications_splits(session)\n",
    "# reset_split(train_cands)\n",
    "# reset_split(dev_cands)\n",
    "# reset_split(test_cands)\n",
    "'''\n",
    "X_train = session.query(Candidate).filter(Candidate.split == 1).all()\n",
    "X_dev   = session.query(Candidate).filter(Candidate.split == 2).all()\n",
    "X_test  = session.query(Candidate).filter(Candidate.split == 3).all()\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_dev))\n",
    "print(len(X_test))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaleup = {\n",
    "    150 :'/data4/jfries/scaleup-samples/docs_150.cands_798.tsv',\n",
    "    500 :'/data4/jfries/scaleup-samples/docs_500.cands_2699.tsv',\n",
    "    1000 :'/data4/jfries/scaleup-samples/docs_1000.cands_5363.tsv',  # 17 s\n",
    "    5000 :'/data4/jfries/scaleup-samples/docs_5000.cands_28028.tsv',\n",
    "    10000 :'/data4/jfries/scaleup-samples/docs_10000.cands_55851.tsv', \n",
    "    20000 :'/data4/jfries/scaleup-samples/docs_20000.cands_111421.tsv',\n",
    "    30000 :'/data4/jfries/scaleup-samples/docs_30000.cands_168844.tsv',\n",
    "    40000 :'/data4/jfries/scaleup-samples/docs_40000.cands_224855.tsv',\n",
    "    50000 :'/data4/jfries/scaleup-samples/docs_50000.cands_282004.tsv', # 14min 6s\n",
    "    60000 :'/data4/jfries/scaleup-samples/docs_60000.cands_337560.tsv',\n",
    "    70000 :'/data4/jfries/scaleup-samples/docs_70000.cands_394021.tsv',\n",
    "    80000 :'/data4/jfries/scaleup-samples/docs_80000.cands_449237.tsv',\n",
    "    90000 :'/data4/jfries/scaleup-samples/docs_90000.cands_504943.tsv',\n",
    "    100000 :'/data4/jfries/scaleup-samples/docs_100000.cands_560198.tsv' \n",
    "}\n",
    "\n",
    "def load_train_sample(fpath):\n",
    "    \"\"\"Load TSV defining documents that comprise our training set\"\"\"\n",
    "    d = [row.strip().split(\"\\t\") for row in open(fpath,'r').read().splitlines()]\n",
    "    return {r[0]:int(r[1]) for r in d}\n",
    "\n",
    "def load_train_candidates(session, fpath, candidate_class):\n",
    "    doc_names = load_train_sample(fpath).keys()\n",
    "    doc_ids = get_doc_ids_by_name(session, doc_names)\n",
    "    return get_cands_by_doc(session, doc_ids, candidate_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 500\n",
    "\n",
    "X_train = load_train_candidates(session, scaleup[NUM_SAMPLES], ImplantComplication)\n",
    "X_dev   = session.query(Candidate).filter(Candidate.split == 2).all()\n",
    "X_test  = session.query(Candidate).filter(Candidate.split == 3).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_docs_by_cands(candidates):\n",
    "    doc_names = set([c[0].get_stable_id().split(\":\")[0] for c in candidates])\n",
    "    return session.query(Document).filter(Document.name.in_(doc_names)).all()\n",
    "    \n",
    "train_docs = get_docs_by_cands(X_train)\n",
    "dev_docs   = get_docs_by_cands(X_dev)\n",
    "test_docs  = get_docs_by_cands(X_test)\n",
    "\n",
    "documents = train_docs + dev_docs + test_docs\n",
    "\n",
    "print(len(train_docs))\n",
    "print(len(dev_docs))\n",
    "print(len(test_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from metal.label_model.baselines import MajorityLabelVoter\n",
    "\n",
    "# mv = MajorityLabelVoter(seed=123)\n",
    "# scores = mv.score(L_dev, L_gold_dev, metric=['precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BRAT Gold Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gold = BratAnnotations(\"/data4/jfries/brat-iaa/\")\n",
    "gold.annotator_summary()\n",
    "\n",
    "'''\n",
    "gold.annotator_agreement(ignore_types=['Header',\"Anatomy\",'Indication'], \n",
    "                         relations_only=True, method='randolph')\n",
    "'''\n",
    "\n",
    "class_map = lambda c: 1 if c[\"Finding\"].attribute(\"PresentPositive\") else 2\n",
    "gold.init_labels(class_map, ['Complication'], verbose=True)\n",
    "\n",
    "Y_dev  = gold.get_labels(X_dev, neg_label=2)\n",
    "Y_test = gold.get_labels(X_test, neg_label=2)\n",
    "\n",
    "print(\"[DEV]  T/F: {} {}\".format(list(Y_dev).count(1), list(Y_dev).count(2)))\n",
    "print(\"[TEST] T/F: {} {}\".format(list(Y_test).count(1), list(Y_test).count(2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_dev  = gold.get_labels(X_dev)\n",
    "Y_test = gold.get_labels(X_test)\n",
    "\n",
    "Y_dev[Y_dev == 0] = 2\n",
    "Y_test[Y_test == 0] = 2\n",
    "\n",
    "print(len(Y_dev))\n",
    "print(len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weak Supervision Labeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "cands_by_doc = collections.defaultdict(list)\n",
    "for c in X_train:\n",
    "    doc_name = c[0].get_stable_id().split(\":\")[0]\n",
    "    cands_by_doc[doc_name].append(c)\n",
    "    \n",
    "for c in X_dev:\n",
    "    doc_name = c[0].get_stable_id().split(\":\")[0]\n",
    "    cands_by_doc[doc_name].append(c)\n",
    "\n",
    "for c in X_test:\n",
    "    doc_name = c[0].get_stable_id().split(\":\")[0]\n",
    "    cands_by_doc[doc_name].append(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract clinical note primitives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_taggers(documents, taggers, ngrams=6, stopwords=[]):\n",
    "    \"\"\" Apply taggers to documents \"\"\"\n",
    "    markup = defaultdict(lambda :defaultdict(list))\n",
    "    for doc in documents: \n",
    "        for name in taggers:\n",
    "            tags = taggers[name].tag(doc, ngrams=ngrams, stopwords=stopwords)\n",
    "            for layer in tags:\n",
    "                markup[doc.name][layer] = tags[layer]\n",
    "    return markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tagger import DatetimeTagger, UMLSTagger, SectionHeaderTagger\n",
    "\n",
    "# target UMLS concepts\n",
    "concepts = {\n",
    "'ACTIVITY' : ['daily_or_recreational_activity'],\n",
    "'CHEMICAL' : ['clinical_drug', 'antibiotic', 'pharmacologic_substance', 'vitamin'],\n",
    "'DISORDER' : ['disease_or_syndrome', 'acquired_abnormality', 'sign_or_symptom', \n",
    "              'injury_or_poisoning', 'congenital_abnormality', \n",
    "              'anatomical_abnormality', 'pathologic_function'],\n",
    "'ANATOMY'  : ['body_part,_organ,_or_organ_component', 'body_location_or_region', \n",
    "              'body_space_or_junction', 'body_system'],\n",
    "'PROCEDURE': ['diagnostic_procedure', 'therapeutic_or_preventive_procedure', \n",
    "              'health_care_activity'],\n",
    "'BACTERIUM': ['bacterium'],\n",
    "'TEMPORAL' : ['temporal_concept']\n",
    "}\n",
    "\n",
    "taggers = {\n",
    "\"umls\"   : UMLSTagger(concepts, data_root=\"../data/supervision/ontologies/UMLS_2014AB/data/\"),\n",
    "\"date\"   : DatetimeTagger(),\n",
    "\"header\" : SectionHeaderTagger()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rwe.utils import load_dict\n",
    "\n",
    "# stopwords\n",
    "sw = load_dict(\"../data/supervision/dicts/stopwords.txt\")\n",
    "sw = sw.union(set(['today', 'per', 'md', 'unknown', 'date', 'add', 'active', 'none', \n",
    "                   'report', 'doc', 'control', 'stopping', 'level', 'tomorrow', \n",
    "                   'ser', 'relief', 'air', 'new', 'take', 'weight', 'skin', 'edema']))\n",
    "\n",
    "tagged_sentences = apply_taggers(train_docs + dev_docs + test_docs, \n",
    "                              taggers, ngrams=6, stopwords=sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Document\n",
    "#\n",
    "\n",
    "def get_note_sign_date(markup, field='T', note_type=None):\n",
    "    \"\"\"\n",
    "    Some notes include footer info of the form:\n",
    "    D: 01/01/2001 08:00 A CT\n",
    "    T: 01/01/2001 09:00 P CT / SPH\n",
    "    \n",
    "    others include\n",
    "    Date: \n",
    "    \n",
    "    TODO: Dates seem to be note_type dependant\n",
    "    TODO: Ask what D and T mean (start/close?)\n",
    "    TODO: How does this date compare to the jittered timestamp in STRIDE?\n",
    "    \"\"\"\n",
    "    sign_date = None\n",
    "    matches = []\n",
    "    for sidx in markup[\"HEADER\"]:\n",
    "        # no header found \n",
    "        if not markup[\"HEADER\"][sidx]:\n",
    "            continue\n",
    "        h1 = markup[\"HEADER\"][sidx][0].get_span()\n",
    "        # for all dates under the matched header, return the max\n",
    "        dates = markup[\"DATETIME\"][sidx] if sidx in markup[\"DATETIME\"] else []\n",
    "        if dates and re.search(\"^\\s*{}[:]\".format(field), h1):\n",
    "            matches.append(dates[0])\n",
    "            \n",
    "    # sometimes multiple dates appear on a single line due to sentence boundary errors\n",
    "    if matches:\n",
    "        sign_date = sorted(matches, key=lambda x:x[-1], reverse=1)[0]\n",
    "        \n",
    "    # select max date from all within document dates\n",
    "    elif markup[\"DATETIME\"]:   \n",
    "        ts = list(itertools.chain.from_iterable(markup[\"DATETIME\"].values()))\n",
    "        ts = [m for m in ts if m[-1]]\n",
    "        if ts:   \n",
    "            sign_date = sorted(ts, key=lambda x:x[-1], reverse=1)[0]\n",
    "            \n",
    "    return sign_date\n",
    "\n",
    "#\n",
    "# Sentences\n",
    "# \n",
    "\n",
    "def get_sentence_markup(sentence, layer, markup):\n",
    "    \"\"\" \"\"\"\n",
    "    doc_name = sentence.document.name\n",
    "\n",
    "    if doc_name not in markup or layer not in markup[doc_name] \\\n",
    "    or sentence.position not in markup[doc_name][layer] \\\n",
    "    or markup[doc_name][layer][sentence.position] == None:\n",
    "        return []\n",
    "    return sorted(markup[doc_name][layer][sentence.position], key=lambda x:x.char_start, reverse=0)\n",
    "\n",
    "def sentence_contains_list(c, threshold=4):\n",
    "    \"\"\" \"\"\"\n",
    "    s = \" \".join([w for w in sent.words if w.strip()]).strip()\n",
    "    tokens = re.split(r'''[;,]''', s)\n",
    "    return len(tokens) > threshold\n",
    "\n",
    "#\n",
    "# Candidates\n",
    "#\n",
    "\n",
    "def is_in_list(c, threshold=4):\n",
    "    \"\"\" If sentence contains a list (defined by number of commas), is this item in the list?\"\"\"\n",
    "    if not sentence_contains_list(c.get_parent(), threshold):\n",
    "        return False\n",
    "    # TODO\n",
    "\n",
    "def is_list_item(s, threshold=4):\n",
    "    \"\"\"\n",
    "    Identfy sentences of the form: \n",
    "        1. Patient underwent surgery. \n",
    "        2. Recovery went well\n",
    "    \"\"\"\n",
    "    text = c.get_parent().text.strip()\n",
    "    return True if re.search(\"^[1-9]+[.)]|â€¢\", text) else False\n",
    "\n",
    "def is_past_tense(c, verb_window=\"left\"):\n",
    "    # VBD\tVerb, past tense\n",
    "    # VBN\tVerb, past participle\n",
    "    past_tense = set(['VBD', 'VBN'])\n",
    "    btw = list(get_between_tokens(c, attrib='pos_tags', case_sensitive=True))\n",
    "    return True if past_tense.intersection(btw) else False\n",
    "\n",
    "\n",
    "#print(get_sentence_markup(X_train[0].get_parent(), \"DISORDER\", markup))\n",
    "#print(get_sentence_markup(X_train[0].get_parent(), \"HEADER\", markup))\n",
    "#print(get_sentence_markup(X_train[0].get_parent(), \"DATE\", markup))\n",
    "\n",
    "doc_ts = {}\n",
    "doc_ts['train'] = {doc.name:get_note_sign_date(tagged_sentences[doc.name]) for doc in train_docs}\n",
    "n = len(doc_ts['train']) - list(doc_ts['train'].values()).count(None)\n",
    "print(\"Extracted {:>5.1f}% ({:>4}/{:>4}) [TRAIN] document timestamps\".format(n / len(doc_ts['train']) * 100, n , len(doc_ts['train'])))\n",
    "\n",
    "doc_ts['dev'] = {doc.name:get_note_sign_date(tagged_sentences[doc.name]) for doc in dev_docs}\n",
    "n = len(doc_ts['dev']) - list(doc_ts['dev'].values()).count(None)\n",
    "print(\"Extracted {:>5.1f}% ({:>4}/{:>4}) [DEV]   document timestamps\".format(n / len(doc_ts['dev']) * 100, n , len(doc_ts['dev'])))\n",
    "\n",
    "doc_ts['test'] = {doc.name:get_note_sign_date(tagged_sentences[doc.name]) for doc in test_docs}\n",
    "n = len(doc_ts['test']) - list(doc_ts['test'].values()).count(None)\n",
    "print(\"Extracted {:>5.1f}% ({:>4}/{:>4}) [TEST]  document timestamps\".format(n / len(doc_ts['test']) * 100, n , len(doc_ts['test'])))\n",
    "\n",
    "doc_ts_all = {}\n",
    "doc_ts_all.update(doc_ts['train'])\n",
    "doc_ts_all.update(doc_ts['dev'])\n",
    "doc_ts_all.update(doc_ts['test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "\n",
    "doctimes = extract_doctimes(documents, tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "DOCTIME_OVERLAP = 1\n",
    "DOCTIME_BEFORE  = 2\n",
    "\n",
    "TRUE = 1\n",
    "FALSE = 2\n",
    "\n",
    "historical_headers = set(['ADMITTING HISTORY','PAST SURGICAL HISTORY', 'CLINICAL HISTORY',\n",
    "                          'PAST MEDICAL HISTORY', 'Past Medical/Surgical History'])\n",
    "present_illness_headers = set(['HISTORY OF PRESENT ILLNESS', 'IMPRESSION', 'DIAGNOSIS', \n",
    "                               'FINDINGS', 'ID/HPI', 'HPI', 'History of Present Illness'])\n",
    "\n",
    "#OPERATION PERFORMED\n",
    "#PROCEDURE IN DETAIL\n",
    "#INDICATIONS\n",
    "\n",
    "historical_headers = set([x.lower() for x in historical_headers])\n",
    "present_illness_headers = set([x.lower() for x in present_illness_headers])\n",
    "\n",
    "#\n",
    "# Helper Functions\n",
    "#\n",
    "def get_relation_head(c):\n",
    "    \"\"\"Get first word (in order of terms) in relation pair\"\"\"\n",
    "    return c[0] if c[0].char_start < c[1].char_start else c[1]\n",
    "\n",
    "# def get_span_entity_type(span, markup):\n",
    "#     \"\"\"Given a span and sentence markup, determine entity type \"\"\"\n",
    "#     pass\n",
    "\n",
    "def overlaps(c, span):\n",
    "    char_start, char_end = span\n",
    "    v = c.char_start >= char_start and c.char_start <= char_end\n",
    "    return v or c.char_end >= char_start and c.char_end <= char_end\n",
    "    \n",
    "    \n",
    "def contained_by(c, span):\n",
    "    char_start, char_end = span\n",
    "    return c.char_start >= char_start and c.char_end <= char_end   \n",
    "\n",
    "\n",
    "def get_span_entities(sentence, span, layer, markup):\n",
    "    \"\"\"Return all entities of type 'layer' in the provided span.\"\"\"\n",
    "    entities = get_sentence_markup(sentence, layer, markup)\n",
    "    if not entities:\n",
    "        return []\n",
    "    return [e for e in entities if contained_by(e, span)]\n",
    "    #return sorted(entities, key=lambda x:x.char_start, reverse=0)\n",
    "\n",
    "def is_hypothetical(c, window=25):\n",
    "    \"\"\" Hypothetical mention detection akin to NegEx \"\"\"\n",
    "    head = c.implant if c.implant.char_start < c.complication.char_start else c.complication\n",
    "    left = \" \".join(get_left_tokens(head, window=window))\n",
    "    rgxs = [r'''\\b(if need be)\\b''',\n",
    "            r'''\\b((if|should)\\s+(you|she|he|be)|(she|he|you)\\s+(might|could|may)\\s*(be)*|if)\\b''',\n",
    "            r'''\\b((possibility|potential|chance|need) (for|of)|potentially)\\b''',\n",
    "            r'''\\b(possible)\\b''',\n",
    "            r'''\\b(candidate for)\\b''',\n",
    "            r'''\\b(assuming)\\s+(you|she|he)\\b'''\n",
    "           ]\n",
    "    for rgx in rgxs:\n",
    "        if re.search(rgx, left, re.I):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_history_of(c, window=25):\n",
    "    \"\"\"Historical mention detection akin to NegEx \"\"\"\n",
    "    head = c.implant if c.implant.char_start < c.complication.char_start else c.complication\n",
    "    left = \" \".join(get_left_tokens(head, window=window))\n",
    "    rgxs = [r'''\\b(h/o|hx|history of)\\b''']\n",
    "    for rgx in rgxs:\n",
    "        m = re.search(rgx, left, re.I)\n",
    "        if m:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_section_header(c):\n",
    "    \"\"\"\n",
    "    What section header does this candidate live under? \n",
    "    \"\"\"\n",
    "    header = get_sentence_markup(c.get_parent(), \"HEADER\", tagged_sentences)\n",
    "    \n",
    "    try:\n",
    "        return (header[0].get_span().replace(\":\", \"\") if header[0] else None) if header else None\n",
    "    except IndexError:\n",
    "        return None\n",
    "\n",
    "def get_doctime_class(c, doc_ts, threshold = 24 * 60 * 60):\n",
    "    \"\"\"\n",
    "    Use DATETIME layer and document timestamp to heuristically determine if\n",
    "    mention occurs during the note (DOCTIME_OVERLAP) or sometime \n",
    "    in the past (DOCTIME_BEFORE). This uses in-sentence dates to determine overlap.\n",
    "    \n",
    "    TODO: cleanup\n",
    "    \"\"\"\n",
    "    if not doc_ts:\n",
    "        return None\n",
    "    sent_ts = get_sentence_markup(c.get_parent(), \"DATETIME\", tagged_sentences)\n",
    "        \n",
    "    if not sent_ts:\n",
    "        return None\n",
    "    sent_ts = [d for d in list(zip(*sent_ts))[-1] if d]\n",
    "    if not sent_ts:\n",
    "        return None\n",
    "    sent_ts = max(sent_ts)\n",
    "    doc_ts = doc_ts[-1]\n",
    "    \n",
    "    if doc_ts == sent_ts:\n",
    "        return DOCTIME_OVERLAP\n",
    "\n",
    "    tdelta = (doc_ts - sent_ts).total_seconds()\n",
    "    return DOCTIME_BEFORE if tdelta > threshold else DOCTIME_OVERLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def misattached_entities2_v2(c):\n",
    "    \"\"\"\n",
    "    Check whether a pain anatomy mention is mis-attached where pain mention precedes anatomy mention\n",
    "    e.g. if note contains 'chest pain, left leg also tender'\n",
    "    and candidate is (pain, left leg)\n",
    "\n",
    "    :param c: pain-anatomy candidate\n",
    "    :return: boolean; True if candidate is misattached, False otherwise\n",
    "    \"\"\"\n",
    "    right_window = get_right_tokens(c, 10)\n",
    "    between_tokens = get_between_tokens(c)\n",
    "    between_phrase = ' '.join(between_tokens).lower()\n",
    "    \n",
    "    #reduced accuracy of LF from 86 to 84, but improved F1\n",
    "    '''\n",
    "    complication_boolean = False\n",
    "    if any(complication in between_phrase for complication in complications) and \"and\" in between_phrase:\n",
    "        complication_boolean = True\n",
    "    '''\n",
    "    of_the_boolean = \"of the\" in between_phrase\n",
    "    due_to_boolean = \"due to\" in between_phrase \n",
    "    \n",
    "    b = c.complication.char_end < c.implant.char_start\n",
    "    b &= list_contains_pain_mention(right_window)\n",
    "    b &= not of_the_boolean\n",
    "    b &= not due_to_boolean\n",
    "    \n",
    "    return True if b else False\n",
    "\n",
    "def misattached_entities4(c):\n",
    "    between_tokens = list(get_between_tokens(c))\n",
    "    right_window = get_right_tokens(c, 14)\n",
    "\n",
    "    b = c.implant.char_end < c.complication.char_start\n",
    "    b &= list_contains_anatomy_mention(right_window)\n",
    "    b &= not list_contains_anatomy_mention(between_tokens)\n",
    "\n",
    "    return True if b else False\n",
    "\n",
    "def misattached_any(c):\n",
    "\n",
    "    b = misattached_entities(c)\n",
    "    b |= misattached_entities2(c)\n",
    "    b |= misattached_entities2_v2(c)\n",
    "    b |= misattached_entities3(c)\n",
    "    b |= misattached_entities4(c)\n",
    "    \n",
    "    return True if b else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New labeling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anatomy_dict = load_dict(\"../data/supervision/dicts/implant_types/anatomy_shc.txt\")\n",
    "\n",
    "implant_dict = load_dict(\"../data/supervision/dicts/implant_types.filtered.txt\")\n",
    "\n",
    "indications = set(['deep vein thrombosis', 'dvt', 'degenerative joint disease', 'narrowing',\n",
    "                   'dvt', 'fracture', 'osteoarthritis', 'avascular necrosis'])\n",
    "\n",
    "complications_all = load_dict(\"../data/supervision/dicts/implant_complications.all.txt\")\n",
    "complications = set(['infection', 'infected', 'wear', 'osteolysis', 'lucency', 'lucencies', 'revision', 'migration'])\n",
    "\n",
    "def LF_anatomy_mention(c):\n",
    "    \"\"\"Mention is in anatomy dictionary\"\"\"\n",
    "    mention = c.implant.get_span()\n",
    "    \n",
    "    complication = c.complication.get_span().lower()\n",
    "    \n",
    "    is_revision = \"revision\" in complication\n",
    "    \n",
    "    v = mention in anatomy_dict\n",
    "    v &= not is_revision\n",
    "    \n",
    "    return -1 if v else 0\n",
    "    \n",
    "def LF_anatomy_as_implant(c):\n",
    "    \"\"\"Anatomy term referring to an implant + clear complication\"\"\"\n",
    "    mention = c.implant.get_span().lower()\n",
    "    if not mention in anatomy_dict:\n",
    "        return 0\n",
    "    keywords = set(['replacement', 'wear', 'heterotopic ossification', \n",
    "                    'mechanical failure', 'migration'])\n",
    "    lemma = \" \".join([w for w in c.complication.get_attrib_tokens('lemmas') if w.strip()])\n",
    "    return -1 if lemma in keywords else 0\n",
    "    \n",
    "def LF_anatomy_revision(c):\n",
    "    \"\"\"Anatomy term referring to an implant + revision\"\"\"\n",
    "    \n",
    "    distance = len(list(get_between_tokens(c)))\n",
    "    \n",
    "    mention = c.implant.get_span().lower()\n",
    "    if not mention in anatomy_dict:\n",
    "        return 0\n",
    "    keywords = set(['revision'])\n",
    "    lemma = \" \".join([w for w in c.complication.get_attrib_tokens('lemmas') if w.strip()])\n",
    "    \n",
    "    v = distance < 3\n",
    "    v &= lemma in keywords\n",
    "    \n",
    "    return 1 if v else 0\n",
    "\n",
    "def LF_anatomy_pain_implant(c):\n",
    "    \"\"\"Hip pain due to infected prosthesis\"\"\"\n",
    "    mention = c.implant.get_span().lower()\n",
    "    complication = c.complication.get_span().lower()\n",
    "\n",
    "    right_window_text = ' '.join(get_right_tokens(c, window=15))\n",
    "    \n",
    "    #if not mention in anatomy_dict:\n",
    "    #    return 0\n",
    "    \n",
    "    #if not \"pain\" in complication:\n",
    "    #    return 0\n",
    "   \n",
    "    implant_boolean  = False\n",
    "\n",
    "    if (any(implant_term in right_window_text for implant_term in implant_dict) or 'prosthesis' in right_window_text) and 'due to' in right_window_text:\n",
    "        implant_boolean = True    \n",
    "    \n",
    "    return 1 if implant_boolean else 0\n",
    "\n",
    "def LF_indication(c):\n",
    "    \"\"\"Anatomy term + common indication\"\"\"\n",
    "    mention = c.implant.get_span().lower()\n",
    "    if not mention in anatomy_dict:\n",
    "        return 0\n",
    "    keywords = set()\n",
    "    lemma = \" \".join([w.lower() for w in c.complication.get_attrib_tokens('lemmas') if w.strip()])\n",
    "    return -1 if lemma in indications else 0\n",
    "\n",
    "def LF_implant_indication(c):\n",
    "    \"\"\"Implant term + common indication\"\"\"\n",
    "    \n",
    "    mention = c.implant.get_span().lower()\n",
    "    \n",
    "    implant_boolean = False\n",
    "    \n",
    "    if any(implant_term in mention for implant_term in implant_dict):\n",
    "        implant_boolean = True\n",
    "    keywords = set()\n",
    "    lemma = \" \".join([w.lower() for w in c.complication.get_attrib_tokens('lemmas') if w.strip()])\n",
    "    \n",
    "    v = implant_boolean\n",
    "    v&= lemma in indications\n",
    "    \n",
    "    return -1 if v else 0\n",
    " \n",
    "def LF_hypothetical(c):\n",
    "    sent_spans = get_sent_candidate_spans(c)\n",
    "    sent = ''\n",
    "    for span in sent_spans:\n",
    "        words = span.get_parent()._asdict()['words']\n",
    "        sent += ' '.join(words)\n",
    "        sent = sent.lower()\n",
    "    \n",
    "    b1 = is_hypothetical(c)\n",
    "    b = False\n",
    "    if 'prevent' in sent or b1:\n",
    "        b = True\n",
    "    return -1 if b else 0\n",
    "\n",
    "def count_implant_mentions_in_doc(c):\n",
    "    doc_name = c[0].get_stable_id().split(\":\")[0]\n",
    "    implant_mention = c.implant.get_span().lower()\n",
    "\n",
    "    implants = []\n",
    "    for cs in cands_by_doc[doc_name]:\n",
    "        implants.append(cs.implant)\n",
    "    \n",
    "    counts = 0\n",
    "    \n",
    "    for cs_implant in implants:\n",
    "        if any(implant_term in cs_implant.get_span().lower() for implant_term in implant_dict):\n",
    "                counts +=1\n",
    "    return counts\n",
    "\n",
    "def count_implant_laterality_mentions_in_doc(c, laterality):\n",
    "    \n",
    "    doc_name = c[0].get_stable_id().split(\":\")[0]\n",
    "\n",
    "    implants = []\n",
    "    for cs in cands_by_doc[doc_name]:\n",
    "        implants.append(cs.implant)\n",
    "    \n",
    "    counts = 0\n",
    "    \n",
    "    for cs_implant in implants:\n",
    "        if any(implant_term in cs_implant.get_span().lower() for implant_term in implant_dict):\n",
    "            if laterality in cs_implant.get_span().lower():\n",
    "                counts +=1\n",
    "    return counts\n",
    "\n",
    "def LF_misattached_entities2_v2(c):\n",
    "    \"\"\"\n",
    "    Check whether a pain anatomy mention is mis-attached where pain mention precedes anatomy mention\n",
    "    e.g. if note contains 'chest pain, left leg also tender'\n",
    "    and candidate is (pain, left leg)\n",
    "\n",
    "    :param c: pain-anatomy candidate\n",
    "    :return: boolean; True if candidate is misattached, False otherwise\n",
    "    \"\"\"\n",
    "        \n",
    "    b = misattached_entities2_v2(c)\n",
    "    return -1 if b else 0\n",
    "\n",
    "def LF_anatomy_implant(c):\n",
    "    doc_name = c[0].get_stable_id().split(\":\")[0]\n",
    "    implant_mention = c.implant.get_span().lower()\n",
    "    \n",
    "    implants = []\n",
    "    for cs in cands_by_doc[doc_name]:\n",
    "        implants.append(cs.implant)\n",
    "    \n",
    "    left_counts = 0\n",
    "    right_counts = 0\n",
    "    \n",
    "    if not implant_mention in anatomy_dict:\n",
    "        return 0\n",
    "    for cs_implant in implants:\n",
    "        if any(implant_term in cs_implant.get_span().lower() for implant_term in implant_dict):\n",
    "                if 'left' in cs_implant.get_span().lower():\n",
    "                    left_counts +=1\n",
    "                if 'right' in cs_implant.get_span().lower():\n",
    "                    right_counts +=1\n",
    "    \n",
    "    b = False\n",
    "    if ('left' in implant_mention and left_counts > 1) or ('right' in implant_mention and right_counts > 1):\n",
    "        b = True\n",
    "    #print(b)\n",
    "    return 1 if b else 0\n",
    "\n",
    "def LF_date(c):\n",
    "    right_window = get_right_tokens(c, window=15)\n",
    "    right_window_text = ' '.join(right_window)\n",
    "    \n",
    "    pattern = '[0-9]{1,2}/[0-9]{1,2}/[0-9]{2,4}'\n",
    "\n",
    "    matcher = re.compile(pattern, flags=re.I)\n",
    "    result = matcher.search(right_window_text)\n",
    "\n",
    "    b = result is not None\n",
    "        \n",
    "    return -1 if b else 0\n",
    "\n",
    "def LF_complication(c):\n",
    "    \"\"\"Implant term + common complication\"\"\"\n",
    "    \n",
    "    distance = len(list(get_between_tokens(c)))\n",
    "    \n",
    "    sent_spans = get_sent_candidate_spans(c)\n",
    "    sent = ''\n",
    "    for span in sent_spans:\n",
    "        words = span.get_parent()._asdict()['words']\n",
    "        sent += ' '.join(words)\n",
    "        sent = sent.lower()\n",
    "\n",
    "    negated_boolean = False\n",
    "    if \"without evidence\" in sent or \"no evidence\" in sent:\n",
    "        negated_boolean = True\n",
    "    \n",
    "    implant_mention = c.implant.get_span().lower()\n",
    "    \n",
    "    implant_boolean = False\n",
    "    if any(implant_term in implant_mention for implant_term in implant_dict):\n",
    "        implant_boolean = True\n",
    "    \n",
    "    lemma = \" \".join([w.lower() for w in c.complication.get_attrib_tokens('lemmas') if w.strip()])\n",
    "\n",
    "    history_of = is_history_of(c)\n",
    "    \n",
    "    misattached = misattached_any(c)\n",
    "        \n",
    "    doc = c.get_parent().document\n",
    "    doc_ts = doctimes[doc.name]\n",
    "    doctime = get_doctime_class(c, doc_ts, tagged_sentences)\n",
    "        \n",
    "    before = False\n",
    "    \n",
    "    # mention occurs before note doctime\n",
    "    if doctime == DOCTIME_BEFORE:\n",
    "        before = True\n",
    "         \n",
    "    v = lemma in complications\n",
    "    v &= distance < 8\n",
    "    v &= not misattached\n",
    "    v &= implant_boolean\n",
    "    v &= not negated_boolean\n",
    "    v &= not history_of \n",
    "    v &= not before\n",
    "    \n",
    "    return 1 if v else 0\n",
    "\n",
    "def LF_misattached_entities4(c):\n",
    "    \n",
    "    b = misattached_entities4(c)\n",
    "    \n",
    "    return -1 if b else 0\n",
    "\n",
    "def LF_misattached_any(c):\n",
    "    \n",
    "    b = misattached_any(c)\n",
    "    \n",
    "    return -1 if b else 0\n",
    "    \n",
    "def LF_contiguous_right_finding(c):\n",
    "    \"\"\"\n",
    "    Check for complication-anatomy candidates that are compound mentions\n",
    "    e.g 'hip revision', 'implant infection'\n",
    "    where finding mention is directly attached to implant mention\n",
    "    and where the mention is not negated (using Negex)\n",
    "\n",
    "    :param c: finding-implant candidate\n",
    "    :return: 1 if True, 0 otherwise\n",
    "    \"\"\"\n",
    "    possible_terms = [x['term'].split(' ') for x in negex.dictionary['definite'] if x['direction'] == 'forward']\n",
    "    longest = len(max(possible_terms, key=len))\n",
    "    window = longest + 2\n",
    "    distance = len(list(get_between_tokens(c)))\n",
    "\n",
    "    history_of = is_history_of(c)\n",
    "    \n",
    "    hypothetical = is_hypothetical(c)\n",
    "    \n",
    "    implant_boolean = False\n",
    "    if any(implant_term in c.implant.get_span().lower() for implant_term in implant_dict):\n",
    "        implant_boolean = True\n",
    "        \n",
    "    fracture_boolean = False\n",
    "    if any(fracture_term in c.complication.get_span().lower() for fracture_term in [\"fracture\", \"nonunion\", \"non-union\"]):\n",
    "        fracture_boolean = True\n",
    "    \n",
    "    removal_boolean = False\n",
    "    if any(removal_term in c.complication.get_span().lower() for removal_term in [\"removal\", \"remove\", \"removed\"]):\n",
    "        fracture_boolean = True\n",
    "    \n",
    "    v = distance < 1\n",
    "    v &= implant_boolean\n",
    "    v &= not fracture_boolean\n",
    "    v &= not history_of\n",
    "    v &= not hypothetical\n",
    "    v &= c.complication.char_end > c.implant.char_end\n",
    "    v &= not negex.is_negated(c.complication, 'definite', 'left', window)\n",
    "    \n",
    "    return 1 if v else 0\n",
    "\n",
    "def LF_contiguous_left_finding(c):\n",
    "    \"\"\"\n",
    "    Check for finding-implant candidates that are compound mentions\n",
    "    e.g 'infection hip'\n",
    "    where finding mention is directly attached to implant mention\n",
    "    and where the mention is not negated (using Negex)\n",
    "\n",
    "    :param c: pain-anatomy candidate\n",
    "    :return: 1 if True, 0 otherwise\n",
    "    \"\"\"\n",
    "        \n",
    "    misattached = misattached_any(c)\n",
    "    \n",
    "    v = len(list(get_between_tokens(c))) < 1\n",
    "    v &= not misattached\n",
    "    v &= c.complication.char_end < c.implant.char_end\n",
    "\n",
    "    return 1 if v else 0\n",
    "\n",
    "def LF_near_contiguous_right_finding(c):\n",
    "    \n",
    "    possible_terms = [x['term'].split(' ') for x in negex.dictionary['definite'] if x['direction'] == 'forward']\n",
    "    longest = len(max(possible_terms, key=len))\n",
    "    left_window_length = longest + 2\n",
    "    left_window = get_left_tokens(c, window=left_window_length)\n",
    "    between_tokens = list(get_between_tokens(c))\n",
    "\n",
    "    f = (lambda w: w.lower())\n",
    "    between_terms = [ngram for ngram in tokens_to_ngrams(\n",
    "        map(f, c.complication.get_parent()._asdict()['words'][c.implant.get_word_start():c.complication.get_word_start() + 1]),\n",
    "        n_max=1)]\n",
    "    between_phrase = ' '.join(between_terms) + ' '\n",
    "\n",
    "    negated_in_between = False\n",
    "\n",
    "    for pt in possible_terms:\n",
    "        pattern = '\\s'.join(pt) + '[-\\s]*'\n",
    "        negated_in_text = regex_in_text(pattern, between_phrase)\n",
    "        if negated_in_text:\n",
    "            negated_in_between = True\n",
    "\n",
    "    odd_list = [\"flexion\", \"regimen\", \"raise\"]\n",
    "    odd = False\n",
    "    for oe in odd_list:\n",
    "        if oe in between_phrase:\n",
    "            odd = True\n",
    "\n",
    "    pain_between = False\n",
    "    for nterm in complications_all:\n",
    "        if nterm in between_tokens or nterm in left_window:\n",
    "            pain_between = True\n",
    "    \n",
    "    misattached = misattached_any(c)\n",
    "    \n",
    "    hypothetical = is_hypothetical(c)\n",
    "                \n",
    "    v = len(between_tokens) < 10\n",
    "    v &= ',' in between_tokens or 'and' in between_tokens\n",
    "    v &= c.complication.char_start > c.implant.char_end\n",
    "    v &= not negex.is_negated(c.implant, 'definite', 'left', left_window_length)\n",
    "    v &= not negated_in_between\n",
    "    v &= not pain_between\n",
    "    v &= not candidate_in_list(c)\n",
    "    v &= not misattached\n",
    "    v &= not hypothetical\n",
    "    v &= not odd\n",
    "\n",
    "    return 1 if v else 0\n",
    "\n",
    "def LF_near_contiguous_left_finding(c):\n",
    "    \"\"\"\n",
    "    Check for finding-implant candidates that are\n",
    "    non-contiguous but close mentions,\n",
    "    e.g., 'pain in the hip', 'tenderness of the right side'\n",
    "\n",
    "    :param c: finding-implant candidate\n",
    "    :return: 1 if True, 0 otherwise\n",
    "    \"\"\"\n",
    "    between_tokens = list(get_between_tokens(c))\n",
    "\n",
    "    left_window_length = 8\n",
    "    negated = negex.is_negated(c, 'definite', 'left', left_window_length)\n",
    "\n",
    "    right_window = get_right_tokens(c, window=3)\n",
    "\n",
    "    pain_in_right_window = list_contains_pain_mention(right_window)\n",
    "\n",
    "    misattached = misattached_any(c)\n",
    "    \n",
    "    v = len(between_tokens) < 3\n",
    "    v &= c.complication.char_end < c.implant.char_start\n",
    "    v &= not misattached\n",
    "    v &= not pain_in_right_window\n",
    "    v &= not negated\n",
    "    v &= not '(' in between_tokens\n",
    "    v &= not ',' in between_tokens\n",
    "\n",
    "    return 1 if v else 0\n",
    "\n",
    "def LF_long_distance_left_finding(c):\n",
    "    \"\"\"\n",
    "    Check for finding-implant candidates that are\n",
    "    long distance mentions,\n",
    "    e.g., 'infection in the left THA'\n",
    "    where candidate is (pain, left ankle)\n",
    "\n",
    "    :param c: pain-anatomy candidate\n",
    "    :return: 1 if True, 0 otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    between_tokens = list(get_between_tokens(c))\n",
    "    right_window = get_right_tokens(c, 3)\n",
    "    left_window_length = 5\n",
    "    left_window = get_left_tokens(c, left_window_length)\n",
    "    \n",
    "    hypothetical = is_hypothetical(c)\n",
    "    \n",
    "    misattached = misattached_any(c)\n",
    "    \n",
    "    v = len(between_tokens) < 10\n",
    "    v &= c.complication.char_end < c.implant.char_start\n",
    "    v &= not list_contains_pain_mention(right_window)\n",
    "    v &= not list_contains_pain_mention(between_tokens)\n",
    "    v &= not list_contains_anatomy_mention(left_window)\n",
    "    v &= not negex.is_negated(c, 'definite', 'left', left_window_length)\n",
    "    v &= not date_between(c)\n",
    "    v &= not candidate_in_list(c)\n",
    "    v &= not left_pain_multiple_anatomy(c)\n",
    "    v &= not hypothetical\n",
    "    v &= not misattached\n",
    "\n",
    "    return 1 if v else 0\n",
    "\n",
    "def LF_right_finding_causing(c):\n",
    "    between_tokens = list(get_between_tokens(c))\n",
    "    \n",
    "    left = \" \".join(get_left_tokens(c.implant, window=5))\n",
    "    \n",
    "    complication_boolean = any(complication in left for complication in complications)\n",
    "\n",
    "    v = \"causing\" in between_tokens\n",
    "    v &=  c.implant.char_end < c.complication.char_start\n",
    "    \n",
    "    return 1 if v else 0\n",
    "\n",
    "def LF_icd_complication(c):\n",
    "    complication = c.complication.get_span().lower()\n",
    "    \n",
    "    v = '996' in complication\n",
    "    \n",
    "    return 1 if v else 0\n",
    "\n",
    "def LF_implant_revision(c):\n",
    "    complication =  c.complication.get_span().lower()\n",
    "    implant = c.implant.get_span().lower()\n",
    "    \n",
    "    revision_boolean = \"revision\" in complication\n",
    "    \n",
    "    implant_boolean = any(implant_term in implant for implant_term in implant_dict)\n",
    "    \n",
    "    misattached = misattached_any(c)\n",
    "    \n",
    "    v = revision_boolean\n",
    "    v &= implant_boolean\n",
    "    v &= not misattached\n",
    "    return 1 if v else 0\n",
    "\n",
    "def LF_implant_bacteria(c):\n",
    "    \n",
    "    implant = c.implant.get_span().lower()\n",
    "\n",
    "    complication = c.complication\n",
    "    \n",
    "    bacterium = get_sentence_markup(c.get_parent(), \"BACTERIUM\", tagged_sentences)\n",
    "        \n",
    "    bacteria_boolean = False\n",
    "    \n",
    "    implant_boolean = any(implant_term in implant for implant_term in implant_dict)\n",
    "    \n",
    "    if bacterium:\n",
    "        for bacteria in bacterium:\n",
    "            if overlaps(complication, (bacteria.char_start, bacteria.char_end)):\n",
    "                bacteria_boolean = True\n",
    "   \n",
    "    misattached = misattached_any(c)\n",
    "    \n",
    "    date = LF_date(c)\n",
    "    \n",
    "    v = bacteria_boolean\n",
    "    v &= implant_boolean\n",
    "    v &= not misattached\n",
    " \n",
    "    return 1 if v else 0\n",
    "\n",
    "def LF_complication_explant(c):\n",
    "    complication = c.complication.get_span().lower()\n",
    "    \n",
    "    v = complication == \"explant\"\n",
    "    \n",
    "    return -1 if v else 0\n",
    "\n",
    "def LF_nonunion(c):\n",
    "    complication = c.complication.get_span().lower()\n",
    "    \n",
    "    v = complication == \"nonunion\" or complication == \"non-union\"\n",
    "    \n",
    "    return -1 if v else 0    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load previous labeling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rwe.labelers import *\n",
    "\n",
    "# get our pain/anatomy relation labeling functions\n",
    "lfs = get_labeling_functions(\"pain_anatomy\")\n",
    "\n",
    "rm = ['LF_contiguous_left_pain', \n",
    "      'LF_contiguous_right_pain', \n",
    "      'LF_near_contiguous_right_pain', \n",
    "      'LF_long_distance_left_pain',\n",
    "      'LF_misattached_entities',\n",
    "      'LF_misattached_entities2',\n",
    "      'LF_misattached_entities3',\n",
    "      'LF_left_pain_anatomy_between'\n",
    "     ]\n",
    "\n",
    "lfs = [lf for lf in lfs if lf.__name__ not in rm]\n",
    "\n",
    "print(\"Loaded {} labeling functions\\n\".format(len(lfs)))\n",
    "for lf in lfs:\n",
    "    print(lf.__name__)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new labeling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lfs += [\n",
    "    LF_anatomy_mention,\n",
    "    LF_indication,\n",
    "    LF_hypothetical, \n",
    "    LF_complication,\n",
    "    LF_implant_indication,\n",
    "    LF_anatomy_revision,\n",
    "    LF_anatomy_pain_implant,\n",
    "    LF_date,\n",
    "    LF_contiguous_right_finding,\n",
    "    LF_contiguous_left_finding,\n",
    "    #LF_long_distance_left_finding,\n",
    "    LF_right_finding_causing,\n",
    "    LF_icd_complication,\n",
    "    LF_implant_revision,\n",
    "    #LF_implant_bacteria,\n",
    "    LF_complication_explant,\n",
    "    LF_nonunion\n",
    "]\n",
    "print(\"Labeling Functions\", len(lfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in X_dev:\n",
    "    v = LF_implant_bacteria(c)\n",
    "    if v:\n",
    "        print('-'*30)\n",
    "        print(v)\n",
    "        print('-'*30)\n",
    "        print(c)\n",
    "        print('-'*30)\n",
    "        print(c.get_parent().text)\n",
    "        print('-'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Labeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mp_lf import mp_apply_lfs\n",
    "\n",
    "L_train = mp_apply_lfs(lfs, X_train, 1)\n",
    "L_dev   = mp_apply_lfs(lfs, X_dev, 1)\n",
    "L_test  = mp_apply_lfs(lfs, X_test, 1)\n",
    "\n",
    "print(\"Label Matrix [TRAIN]\", L_train.shape)\n",
    "print(\"Label Matrix [DEV]  \", L_dev.shape)\n",
    "print(\"Label Matrix [TEST] \", L_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fix label assignments\n",
    "L_train[L_train==-1] = 2\n",
    "L_dev[L_dev==-1] = 2\n",
    "L_test[L_test==-1] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from metal.analysis import lf_summary, view_label_matrix, view_overlaps\n",
    "\n",
    "lf_summary(L_dev, Y_dev, lf_names = [lf.__name__ for lf in lfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "view_label_matrix(L_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from metal.analysis import view_conflicts\n",
    "view_conflicts(L_train, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Vote Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def majority_vote(L):\n",
    "    '''Majority vote'''\n",
    "    \n",
    "    pred = L.sum(axis=1)\n",
    "    pred[(pred > 0).nonzero()[0]] = 1\n",
    "    pred[(pred < 0).nonzero()[0]] = 0\n",
    "    return pred\n",
    "\n",
    "L_dev_hat = copy.deepcopy(L_dev)\n",
    "L_dev_hat[L_dev_hat==2] = -1\n",
    "Y_dev_pred = majority_vote(L_dev_hat)\n",
    "\n",
    "errors = gold.score(X_dev, Y_dev_pred, ignore_attributes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "sv = SentenceNgramViewer(errors['fn'], session=session, n_per_page=1, height=225, annotator_name='gold')\n",
    "sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected = sv.get_selected()\n",
    "\n",
    "print(selected.get_parent().document.name)\n",
    "\n",
    "for lf in lfs:\n",
    "    v = lf(selected)\n",
    "    if v != 0:\n",
    "        print(v, lf.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(get_section_header(selected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MeTaL Generative Model\n",
    "IGNORE FOR NOW\n",
    "\n",
    "This uses the new Snorkel MeTaL code for learning LF accuracies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from metal.label_model import LabelModel\n",
    "label_model = LabelModel(k=2, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "label_model.train(L_train, Y_dev=Y_dev, n_epochs=100, print_every=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(label_model.mu.shape)\n",
    "print(label_model.mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# noise-aware\n",
    "scores = label_model.score(L_dev, Y_dev, metric=['precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from metal.label_model.baselines import MajorityLabelVoter\n",
    "# majority vote\n",
    "scores = mv.score(L_dev, Y_dev, metric=['precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling Function Empirical Accuracy Statistics\n",
    "Since we have labeled development data, we can examine empirical statistics for labeling function performance. Good labeling function design requires than any heuristic be correct with probablity better than random chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#L_dev.lf_stats(session, labels=L_gold_dev.toarray().ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Generative Model\n",
    "Grid search for tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from scipy.sparse import load_npz\n",
    "from snorkel.learning import GenerativeModel\n",
    "from snorkel.learning import RandomSearch\n",
    "\n",
    "# use random search to optimize the generative model\n",
    "param_ranges = {\n",
    "    'step_size' : [1e-3, 1e-4, 1e-5, 1e-6],\n",
    "    'decay'     : [0.9, 0.95],\n",
    "    'epochs'    : [100, 500],\n",
    "    'reg_param' : [1e-4],\n",
    "}\n",
    "model_class_params = {'lf_propensity' : True}\n",
    "\n",
    "searcher = RandomSearch(GenerativeModel, param_ranges, L_train, n=5, model_class_params=model_class_params)\n",
    "%time gen_model, run_stats = searcher.fit(L_dev, L_gold_dev)\n",
    "run_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from snorkel.annotations import load_marginals, save_marginals\n",
    "\n",
    "# train_marginals = gen_model.marginals(L_train)\n",
    "# save_marginals(session, L_train, train_marginals, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Labeling Function Accuracy Weights\n",
    "These are the accuracy factor weights learned during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lf_accs = []\n",
    "for name,acc in zip([lf.__name__ for lf in lfs], gen_model.weights.lf_accuracy):\n",
    "    lf_accs.append({\"LF-NAME\":name, \"Acc. Factor Weight\":acc})\n",
    "pd.DataFrame(lf_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 6))\n",
    "df = pd.DataFrame(data=train_marginals, columns=['marginals'])\n",
    "pd.DataFrame.hist(df,range=(0.0, 1.0),bins=20, ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
