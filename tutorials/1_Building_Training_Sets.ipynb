{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Training Sets with Weak Supervision\n",
    "In this tutorial, we'll build a `Pain-Anatomy` relation training set using weakly superivsed methods. This notebook covers: \n",
    "- Loading pre-processed documents\n",
    "- Generating relational candidates \n",
    "- Applying labeling functions\n",
    "- Training a Snorkel Label Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'../../ehr-rwe/')\n",
    "\n",
    "import glob\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version 3.6.7 | packaged by conda-forge | (default, Nov  6 2019, 16:03:31) \n",
      "[GCC Clang 9.0.0 (tags/RELEASE_900/final)]\n",
      "Snorkel v0.9.4+dev\n",
      "NumPy v1.18.1\n"
     ]
    }
   ],
   "source": [
    "import snorkel\n",
    "\n",
    "print(f'Python version {sys.version}')\n",
    "print(f'Snorkel v{snorkel.__version__}')\n",
    "print(f'NumPy v{np.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load MIMIC-III Documents\n",
    "\n",
    "This notebook assumes documents have already been preprocessed and dumped into JSON format, and placed in the `../data/` subdirectory. We created a small annotated dataset using MIMIC-III patient notes, and provide the relevant row IDs for those notes in `../data/annotations/`. See `tutorials/README.md` for details, and the `mimic_to_tsv.py` and `preprocess.py` scripts in `preprocessing/` to create the required JSON files.\n",
    "\n",
    "You will need access to MIMIC-III data to run this notebook using our tutorial annotations.  See https://mimic.physionet.org/gettingstarted/access/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 55 documents\n",
      "Loaded 1322 documents\n"
     ]
    }
   ],
   "source": [
    "from rwe import dataloader\n",
    "\n",
    "inputdir = '../data/corpora/'\n",
    " \n",
    "corpus = [\n",
    "    dataloader([f'{inputdir}/mimic_gold.0.json']), \n",
    "    dataloader([f'{inputdir}/mimic_unlabeled.0.json'])\n",
    "]\n",
    "\n",
    "for split in corpus:\n",
    "    print(f'Loaded {len(split)} documents')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Candidates\n",
    "\n",
    "This is an example pipeline for generating `Pain-Anat` relation candidates. Relations are defined as a tuple $k$ entity spans. For simplicity's sake, we consider binary relations between all `Anatomy` and `Pain` entity pairs found within the same sentence. Entities can be tagged using a clinical named entity recognition (NER) model if available. Here we use a dictionary-based method to tag our initial `Anatomy` and `Pain` entities. \n",
    "\n",
    "### Clinical Text Markup\n",
    "When writing labeling functions, it's helpful to have access to document markup and other metadata. For example, we might want to know what document section we are currently in (e.g., Past Medical History) or if we have temporal information above an event, such as a data of occurence, we might want to incorporate that information into our labeling heuristics. \n",
    "\n",
    "We have written taggers that identify these document attributes, and execute them below in the same pipeline that extracts our `Pain-Anat` relation candidates.\n",
    "\n",
    "### Timing Benchmarks \n",
    "\n",
    "- 4 core MacBook Pro 2.5Ghz mid-2015\n",
    "\n",
    "| N Documents   | N Cores | Time |\n",
    "|---------------|---------|----------------|\n",
    "| 1322          | 4       | 1 minute 10 secs |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwe.utils import load_dict\n",
    "from rwe.labelers.taggers import (\n",
    "    ResetTags, RelationTagger, \n",
    "    DictionaryTagger, NegExTagger, HypotheticalTagger, HistoricalTagger,\n",
    "    SectionHeaderTagger, ParentSectionTagger,\n",
    "    DocTimeTagger, MappedDocTimeTagger, \n",
    "    Timex3Tagger, Timex3NormalizerTagger, TimeDeltaTagger,\n",
    "    FamilyTagger\n",
    ")\n",
    "\n",
    "dict_pain = load_dict('../data/supervision/dicts/pain/pain.txt')\n",
    "dict_anat = load_dict('../data/supervision/dicts/anatomy/fma_anatomy.bz2')\n",
    "\n",
    "target_entities = ['pain']\n",
    "\n",
    "# NOTE: Pipelines are *order dependant* as normalizers and attribute taggers assume\n",
    "# the existence of certain concept targets (e.g., Timex3Normalizer requires timex3 entities)\n",
    "pipeline = {\n",
    "    # 1. Clear any previous runs\n",
    "    \"reset\"        : ResetTags(),\n",
    "    \n",
    "    # 2. Clinical concepts\n",
    "    \"concepts\"  : DictionaryTagger({'pain': dict_pain, 'anatomy': dict_anat}),\n",
    "    \"headers\"   : SectionHeaderTagger(),\n",
    "    \"timex3\"    : Timex3Tagger(),\n",
    "    \n",
    "    # 3. Normalize datetimes\n",
    "    \"doctimes\"  : DocTimeTagger(prop='CHARTDATE'), # document time stamp\n",
    "    \"normalize\" : Timex3NormalizerTagger(),\n",
    "    \n",
    "    # 4. Concept attributes\n",
    "    \"section\"      : ParentSectionTagger(targets=target_entities),\n",
    "    \"tdelta\"       : TimeDeltaTagger(targets=target_entities),\n",
    "    \"negation\"     : NegExTagger(targets=target_entities, \n",
    "                                 data_root=\"../data/supervision/dicts/negex/\"),\n",
    "    \"hypothetical\" : HypotheticalTagger(targets=target_entities),\n",
    "    'historical'   : HistoricalTagger(targets=target_entities),\n",
    "    'family'       : FamilyTagger(targets=target_entities),\n",
    "    \n",
    "    # 5. Extract relation candidates\n",
    "    \"pain-anat\"    : RelationTagger('pain-anatomy', ('pain', 'anatomy'))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto block size=345\n",
      "Partitioned into 4 blocks, [342 345] sizes\n",
      "CPU times: user 2.38 s, sys: 396 ms, total: 2.77 s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from rwe.labelers import TaggerPipelineServer\n",
    "\n",
    "tagger = TaggerPipelineServer(num_workers=4)\n",
    "documents = tagger.apply(pipeline, corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: 0 n_candidates: 390\n",
      "Split: 1 n_candidates: 5185\n"
     ]
    }
   ],
   "source": [
    "from rwe.utils import build_candidate_set\n",
    "\n",
    "Xs = [\n",
    "    build_candidate_set(documents[0], \"pain-anatomy\"),\n",
    "    build_candidate_set(documents[1], \"pain-anatomy\")\n",
    "]\n",
    "\n",
    "# Split: 0 n_candidates: 390\n",
    "# Split: 1 n_candidates: 5185\n",
    "for i in range(len(Xs)):\n",
    "    print(f'Split: {i} n_candidates: {len(Xs[i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'section': Span(Chief Complaint), 'hist': 1, 'family/other': 1}\n",
      "F s/p mechanical fall frp, standing, no LOC, found by daughter c/o L hip pain, GCS=15\n",
      "---\n",
      "{'section': Span(History of Present Illness), 'family/other': 1}\n",
      "Found after 1.5 hours by her daughter c/o L hip pain.\n",
      "---\n",
      "{'section': Span(Action), 'family/other': 1}\n",
      "Action:    Husband asked staff re: current pain regimen and sleep med plan.\n",
      "---\n",
      "{'section': None, 'family/other': 1}\n",
      "Pregnancy was uncomplicated until this morning, when mother presented with severe abdominal pain.\n",
      "---\n",
      "{'section': None, 'family/other': 1}\n",
      "Pregnancy was uncomplicated until this morning, when mother presented with severe abdominal pain.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for x in Xs[1]:\n",
    "    if 'family/other' in x.pain.props:\n",
    "        print(x.pain.props)\n",
    "        print(x.pain.sentence.text)\n",
    "        print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Gold Labeled Data\n",
    "As a source of gold labeled data for evaluating our relation extraction model, we have annotated MIMIC III notes to identify pain-anatomy relations, using Brat. We have provided the Brat annotation files as part of this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwe.contrib.brat import *\n",
    "\n",
    "inputdir = \"../data/brat/\"\n",
    "\n",
    "gold = BratAnnotations(inputdir)\n",
    "gold.annotator_summary()\n",
    "gold.annotator_agreement(ignore_types=['Concept'], relations_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are only considering `pain-anat` relations for this tutorial, so let's filter the gold data to only pain mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annos = gold.aggregate_raters(ignore_types=['Concept'], relations_only=True)\n",
    "\n",
    "Ys_brat = {}\n",
    "relations = [x for x in annos if x.type == 'X-at']\n",
    "for x in relations:\n",
    "    # only include gold relations for pain-anat\n",
    "    args = [a.text.lower() for a in x.args]\n",
    "    if dict_pain.intersection(args):   \n",
    "        # create unique key DOC_NAME, SPANS\n",
    "        key = tuple([x.doc_name] + sorted([ety.span for ety in x.args]))\n",
    "        Ys_brat[key] = x\n",
    "        \n",
    "print(len(Ys_brat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build our target labels `Ys` mapping to whether each relation is a a true present positive occurance of pain or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = {}\n",
    "\n",
    "Ys = []\n",
    "for x in Xs[0]:\n",
    "    spans = [\n",
    "        ((x.pain.abs_char_start, x.pain.abs_char_end + 1),),\n",
    "        ((x.anatomy.abs_char_start, x.anatomy.abs_char_end + 1),)\n",
    "    ]\n",
    "    key = tuple([x.pain.sentence.document.name] + sorted(spans))\n",
    "    y = 1 if key in Ys_brat else 2\n",
    "    Ys.append(y)\n",
    "    \n",
    "    if key in Ys_brat:\n",
    "        debug[key] = x\n",
    "    \n",
    "print(Ys.count(1)) \n",
    "print(Ys.count(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Candidates\n",
    "When using a simple dictionary-based method to enumerate candidate relations, we typically have some subset of true mentions that we fail to generate. In this case, we are missing anatomical entities such as \"bilateral flank\" and \"midback\", which are not in our UMLS-based anatomy dictionary. These missing candidates impact our final recall score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_missing = 0\n",
    "for key in Ys_brat:\n",
    "    if key not in debug:\n",
    "        print(Ys_brat[key])\n",
    "        n_missing += 1\n",
    "        \n",
    "print(f'Missed {n_missing} ({n_missing / len(Ys_brat) * 100:2.1f}%) of true relations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply Labeling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a set of example labeling functions that use document attributes and sentence content to vote on whether a given `pain-anat` relation candidate is a true mention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from rwe.helpers import get_left_span, get_right_span, get_between_span, token_distance\n",
    "\n",
    "def dict_matches(span, dictionary):\n",
    "    matches = []\n",
    "    toks = span.get_attrib_tokens('words')\n",
    "    for i in range(len(toks)):\n",
    "        for j in range(i+1, len(toks)):\n",
    "            term = ' '.join(toks[i:j]).lower()\n",
    "            if term in dictionary:\n",
    "                matches.append(term)\n",
    "    return matches\n",
    "\n",
    "ABSTAIN  = 0\n",
    "NEGATIVE = 2\n",
    "POSITIVE = 1\n",
    "\n",
    "neg_rgx = re.compile(\n",
    "    r'''\\b(insensitivity|paresthesias|paresthesia|sensitivity|tenderness|discomfort|heaviness|sensitive|itchiness|tightness|throbbing|numbness|tingling|cramping|coldness|soreness|painful|hurting|itching|burning|tender|buring|aching|hurts|aches|pains|hurt|pain|ache|achy|numb)\\b''',\n",
    "    re.I\n",
    ")\n",
    "\n",
    "def LF_is_negated(x):\n",
    "    return NEGATIVE if 'negated' in x.pain.props else ABSTAIN\n",
    "\n",
    "def LF_is_hypothetical(x):\n",
    "    return NEGATIVE if 'hypothetical' in x.pain.props else ABSTAIN\n",
    "\n",
    "def LF_is_historical(x):\n",
    "    if 'hist' not in x.pain.props:\n",
    "        return ABSTAIN\n",
    "    return NEGATIVE if x.pain.props['hist'] == 1 else POSITIVE\n",
    "\n",
    "def LF_section_headers(x):\n",
    "    \"\"\"Predict label based on the section this candidate relation is found in.\"\"\"\n",
    "    sections = {\n",
    "        'past medical history': NEGATIVE,\n",
    "        'chief complaint': POSITIVE,\n",
    "        'discharge instructions': NEGATIVE,\n",
    "        'discharge condition': NEGATIVE,\n",
    "        'active issues':POSITIVE,\n",
    "        'brief hospital course':NEGATIVE,\n",
    "    }\n",
    "    header = x.pain.props['section'].text.lower() if x.pain.props['section'] else None\n",
    "    return ABSTAIN if header not in sections else sections[header]\n",
    "\n",
    "def LF_contiguous_args(x):\n",
    "    \"\"\"Candidate is of the form 'chest pain'\"\"\"\n",
    "    v = not get_between_span(x.pain, x.anatomy)\n",
    "    v &= not 'negated' in x.pain.props\n",
    "    v &= not 'hypothetical' in x.pain.props\n",
    "    v &= not ('hist' in x.pain.props and x.pain.props['hist'] == 1)\n",
    "    v &= not ('tdelta' in x.pain.props and x.pain.props['tdelta'] < -1)\n",
    "    return POSITIVE if v else ABSTAIN\n",
    "\n",
    "def LF_distant_args(x, max_toks=10):\n",
    "    \"\"\"Reject candidate if the arguments occur too far apart (in token distance).\"\"\"\n",
    "    span = get_between_span(x.pain, x.anatomy)\n",
    "    n_toks = len(span.get_attrib_tokens('words')) if span else 0\n",
    "    return NEGATIVE if n_toks > max_toks else ABSTAIN\n",
    "    \n",
    "def LF_between_terms(x):\n",
    "    \"\"\"Reject if some key terms occur between arguments.\"\"\"\n",
    "    span = get_between_span(x.pain, x.anatomy)\n",
    "    if not span:\n",
    "        return ABSTAIN\n",
    "    # negation term      \n",
    "    flag = neg_rgx.search(span.text) is not None\n",
    "    # anatomical term \n",
    "    flag |= len(dict_matches(span, dict_anat)) > 0\n",
    "    return NEGATIVE if flag else ABSTAIN\n",
    "\n",
    "def LF_complains_of(x):\n",
    "    \"\"\"Search for pattern 'complains of X' \"\"\"\n",
    "    rgx = re.compile(r'''\\b(complain(s*|ing*) of)\\b''', re.I)\n",
    "    is_negated = 'negated' in x.pain.props\n",
    "    is_complains_of = rgx.search(get_left_span(x.pain).text) is not None\n",
    "    return POSITIVE if not is_negated and is_complains_of else ABSTAIN\n",
    "\n",
    "def LF_denies(x):\n",
    "    \"\"\"Patient denies X\"\"\"\n",
    "    rgx = re.compile(r'''\\b(denied|denies|deny)\\b''', re.I)\n",
    "    is_denies = rgx.search(get_left_span(x.pain).text) is not None\n",
    "    return NEGATIVE if is_denies else ABSTAIN\n",
    "\n",
    "def LF_non(x):\n",
    "    \"\"\"non-tender\"\"\"\n",
    "    rgx = re.compile(r'''(non)[-]*''', re.I)\n",
    "    is_negated = rgx.search(get_left_span(x.pain, window=2).text) is not None\n",
    "    return NEGATIVE if is_negated else ABSTAIN\n",
    "\n",
    "def LF_if(x):\n",
    "    rgx = re.compile(r'''^if you\\b''', re.I)\n",
    "    is_hypo = rgx.search(get_left_span(x.pain).text) is not None\n",
    "    return NEGATIVE if is_hypo else ABSTAIN    \n",
    "\n",
    "def LF_tdelta(x):\n",
    "    if 'tdelta' not in x.pain.props:\n",
    "        return ABSTAIN\n",
    "    #tok_dist = token_distance(x.pain, x.pain.props['timex_span'])\n",
    "    return NEGATIVE if x.pain.props['tdelta'] < -1 else ABSTAIN\n",
    "\n",
    "def LF_pseudo_negation(x):\n",
    "    rgx = re.compile(r'''\\b(no (change|improvement|difference))\\b''', re.I)\n",
    "    is_negated = rgx.search(get_left_span(x.pain).text) is not None\n",
    "    return POSITIVE if is_negated else ABSTAIN\n",
    "    \n",
    "\n",
    "lfs = [\n",
    "    LF_is_negated,\n",
    "    LF_is_hypothetical,\n",
    "    LF_is_historical,\n",
    "    LF_section_headers,\n",
    "    LF_contiguous_args,\n",
    "    LF_distant_args,\n",
    "    LF_between_terms,\n",
    "    LF_complains_of,\n",
    "    LF_denies,\n",
    "    LF_non,\n",
    "    LF_if,\n",
    "    LF_tdelta,\n",
    "    LF_pseudo_negation\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply these LFs to our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from rwe.labelers import LabelingServer\n",
    "\n",
    "labeler = LabelingServer(num_workers=4)\n",
    "Ls = labeler.apply(lfs, Xs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then examine the accuracy of each of our LFs using the gold labeled data as ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwe.analysis import lf_summary\n",
    "\n",
    "lf_summary(Ls[0], Y=Ys, lf_names=[lf.__name__ for lf in lfs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwe.visualization.analysis import view_conflicts, view_label_matrix, view_overlaps\n",
    "\n",
    "view_overlaps(Ls[1], normalize=False)\n",
    "view_label_matrix(Ls[1])\n",
    "view_conflicts(Ls[1], normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Snorkel Label Model \n",
    "\n",
    "### The next step is to train a Snorkel Label model using the data labeled with our labeling functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sparse matrix to new Snorkel format\n",
    "\n",
    "def convert_label_matrix(L):\n",
    "    L = L.toarray().copy()\n",
    "    L[L == 0] = -1\n",
    "    L[L == 2] = 0\n",
    "    return L\n",
    "\n",
    "\n",
    "Ls_hat = [\n",
    "    convert_label_matrix(Ls[0]),\n",
    "    convert_label_matrix(Ls[1]),\n",
    "]\n",
    "\n",
    "Ys_hat = [\n",
    "    np.array([0 if y == 2 else 1 for y in Ys]),\n",
    "    []\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LabelModel\n",
    "lr = 0.01\n",
    "l2 = 0.01\n",
    "prec_init = 0.7\n",
    "\n",
    "label_model = LabelModel(cardinality=2, device='cpu', verbose=True)\n",
    "label_model.fit(L_train=Ls_hat[1], \n",
    "                n_epochs=100, \n",
    "                lr=lr,\n",
    "                l2=l2,\n",
    "                prec_init=prec_init,\n",
    "                optimizer='adam',\n",
    "                log_freq=100)\n",
    "\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "label_model.score(L=Ls_hat[0], Y=Ys_hat[0], metrics=metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predict Probabalisitic Labels for Unlabeled Data\n",
    "\n",
    "### The last step in this tutorial is to use our labeling model to predict on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_proba = label_model.predict_proba(Ls_hat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = label_model.predict(Ls_hat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Error Analysis\n",
    "This replicates the metrics generated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def apply_lfs(x, lfs):\n",
    "    V = []\n",
    "    for lf in lfs:\n",
    "        v = lf(x)\n",
    "        if v != 0:\n",
    "            V.append((v, lf.__name__))\n",
    "            \n",
    "    return list(zip(*V)) if V else None,None\n",
    "        \n",
    "errs = {}\n",
    "y_gold, y_pred = [],[]\n",
    "for i,(x, y_hat) in enumerate(zip(Xs[0], Y_pred)):\n",
    "    # skip uncovered points \n",
    "    # NOTE: there is currently a bug in the Snorkel label model\n",
    "    if y_hat == -1:\n",
    "        continue\n",
    "        \n",
    "    y_hat = 2 if y_hat == 0 else 1\n",
    "    y_gold.append(Ys[i])\n",
    "    y_pred.append(y_hat)\n",
    "    if y_hat != Ys[i]:\n",
    "        errs[x] = (Ys[i], y_hat, Y_proba[i])\n",
    "    \n",
    "    \n",
    "p = precision_score(y_gold, y_pred)\n",
    "r = recall_score(y_gold, y_pred)\n",
    "f1 = f1_score(y_gold, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p, r, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in errs:\n",
    "    y, y_hat, y_proba = errs[x]\n",
    "    v, fired_lfs = apply_lfs(x,lfs)\n",
    "    print(x)\n",
    "    print('DOCTIME', x.pain.sentence.document.props['doctime'])\n",
    "    print([x.pain.sentence.text])\n",
    "    print(x.pain.props)\n",
    "    print(v)\n",
    "    print(y, y_hat, y_proba)\n",
    "    print('=' * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rwe] *",
   "language": "python",
   "name": "conda-env-rwe-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
